{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Policy-Based REINFORCE method\n",
    "\n",
    "## New implementations\n",
    "\n",
    "- Change matrix values to log2 // binary representation\n",
    "- Change reward = # of merges + log2(new_max fo the matrix) and standardize them at the end of each batch\n",
    "- Change network architecture\n",
    "- Optimizer = RMSProp\n",
    "- Number of episodes 200k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import defaultdict, namedtuple\n",
    "os.chdir(\"/Users/davidamat/Documents/projects/2048/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/davidamat/Documents/projects/2048/src',\n",
       " '/Users/davidamat/Documents/projects/2048/src',\n",
       " '/Users/davidamat/Documents/Glovo/missing-items-pipeline',\n",
       " '/Users/davidamat/opt/anaconda3/lib/python38.zip',\n",
       " '/Users/davidamat/opt/anaconda3/lib/python3.8',\n",
       " '/Users/davidamat/opt/anaconda3/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/Users/davidamat/.local/share/virtualenvs/2048-TNu5m-4D/lib/python3.8/site-packages',\n",
       " '/Users/davidamat/.local/share/virtualenvs/2048-TNu5m-4D/lib/python3.8/site-packages/IPython/extensions',\n",
       " '/Users/davidamat/.ipython']"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import constants as c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Movements():\n",
    "    \n",
    "    @staticmethod\n",
    "    def displacement_numbers(mat):\n",
    "        \"\"\"\n",
    "        Desplaza todos los numeros a la izquierda de la matriz\n",
    "        \"\"\"\n",
    "        \n",
    "        new = np.zeros(mat.shape) # crea una matrix de 0 de AxA (donde A es la c.GRID_LEN)\n",
    "        done = False\n",
    "        for i in range(c.GRID_LEN):\n",
    "            count = 0\n",
    "            for j in range(c.GRID_LEN):\n",
    "                # Si en esa fila hay un elemento no nulo\n",
    "                if mat[i][j] != 0:\n",
    "                    # Pone ese elemento en la posicion count (columna), donde esta vendra determinada por si\n",
    "                    # en esa fila, previamente, hemos encotrado algun otro elemento NO nulo\n",
    "                    new[i][count] = mat[i][j]\n",
    "\n",
    "                    # Solo con que modifiquemos la posicion de una de los numeros, ya contara como un movimiento: done = True\n",
    "                    if j != count:\n",
    "                        done = True\n",
    "\n",
    "                    # Suma al count una posicion ya que ya se ha movido a la izquerda de todo (columna 0) ese elemento\n",
    "                    count += 1\n",
    "        return (new, done)\n",
    "    \n",
    "    @staticmethod\n",
    "    def merge_numbers(mat, game_score = 0):\n",
    "        \"\"\"\n",
    "        Sum consecutive equal numbers (x) in a row\n",
    "        ending with the second position being 0\n",
    "        and the first position being the double of the number x\n",
    "        Return also the number of merges done in all the matrix\n",
    "        \"\"\"\n",
    "        num_merges = 0\n",
    "        for i in range(c.GRID_LEN):\n",
    "            for j in range(c.GRID_LEN-1): # la ultima columna j, no tiene una columna a su derecha (j+1)\n",
    "                celda = mat[i][j]\n",
    "                celda_derecha = mat[i][j+1]\n",
    "                if celda == celda_derecha and celda != 0: # si son iguales, y esta igualdad son numeros > 0, se suman\n",
    "                    merge_val = celda + celda_derecha\n",
    "                    mat[i][j] = merge_val\n",
    "                    game_score += merge_val #suma los puntos\n",
    "                    mat[i][j+1] = 0 # se deja la de la derecha vacía, esto hace que se necesite hacer otro displacement para llenar ese hueco\n",
    "                    # esto tambien hace que al leer la siguiente columna, se lea un 0, y no el numero que estaba\n",
    "                    num_merges += 1\n",
    "        return (mat, num_merges, game_score)\n",
    "    \n",
    "    @staticmethod\n",
    "    def perform_movement(game, game_score):\n",
    "        \"\"\"\n",
    "        1 - mover al maximo a la izquierda todos los numeros\n",
    "        2 - Sumamos los iguales dejando 0 en el segundo sumando\n",
    "        3 - Por seguridad, por si hemos dejado alguno sin desplazar a la izquierda (huecos generados por el merge),\n",
    "            se vuelve a aplicar sin importar el done o no (no importa si mueve a alguien o no ahora)\n",
    "        \"\"\"\n",
    "        game_disp, done_disp = Movements.displacement_numbers(game)\n",
    "        game_merged, num_merges, game_score = Movements.merge_numbers(game_disp, game_score)\n",
    "        game_final = Movements.displacement_numbers(game_merged)[0]\n",
    "        return (game_final, done_disp, num_merges, game_score)\n",
    "    \n",
    "    @staticmethod\n",
    "    def ro(mat, cw = True, num = 1): #cw: clockwise: True or False, #num: number of rotations\n",
    "        \"\"\"\n",
    "        Rota 90º las matrices para que las operaciones UP, DOWN y RIGHT se puedan hacer con la de LEFT\n",
    "        \"\"\"\n",
    "        param_clockise = (1,0) if cw else (0,1) #clockwise or counter-clockwise (see help(np.rot90))\n",
    "\n",
    "        # Cuantas rotaciones hacemos\n",
    "        rot_mat = mat\n",
    "        for _ in range(num):\n",
    "            rot_mat = np.rot90(np.array(rot_mat), axes = param_clockise)\n",
    "\n",
    "        return rot_mat\n",
    "\n",
    "    @staticmethod\n",
    "    def left(game, game_score = 0):\n",
    "        return Movements.perform_movement(game, game_score)\n",
    "    \n",
    "    @staticmethod\n",
    "    def down(game, game_score = 0):\n",
    "        \"\"\"\n",
    "        C - L - UC\n",
    "        \"\"\"\n",
    "        rotate_game = Movements.ro(game) #rotate clockwise\n",
    "        left_game, done, num_merges, game_score = Movements.left(rotate_game, game_score) #apply left\n",
    "        game_final = Movements.ro(left_game, cw = False) #undo the rotation\n",
    "        return game_final, done, num_merges, game_score\n",
    "    \n",
    "    @staticmethod\n",
    "    def up(game, game_score = 0):\n",
    "        \"\"\"\n",
    "        UC - L - C\n",
    "        \"\"\"\n",
    "        rotate_game = Movements.ro(game, cw = False) #rotate anti-clockwise\n",
    "        left_game, done, num_merges, game_score = Movements.left(rotate_game, game_score) #apply left\n",
    "        game_final = Movements.ro(left_game) #undo the rotation\n",
    "        return game_final, done, num_merges, game_score\n",
    "\n",
    "    @staticmethod\n",
    "    def right(game, game_score = 0):\n",
    "        \"\"\"\n",
    "        C - C - L - UC - UC\n",
    "        \"\"\"\n",
    "        rotate_game = Movements.ro(game,cw=True, num =2) #double rotation\n",
    "        left_game, done, num_merges, game_score = Movements.left(rotate_game, game_score) #apply left\n",
    "        game_final = Movements.ro(left_game, cw = False, num = 2) #undo the rotation\n",
    "        return game_final, done, num_merges, game_score\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env():\n",
    "    \n",
    "    def __init__(self, grid_size):\n",
    "        \"\"\"\n",
    "        grid_size: size of the matrix\n",
    "        \"\"\"\n",
    "        # Status\n",
    "        self.game_stat = -2 # 0: playing, 1: win, -1: lost, -2: ready to start\n",
    "        \n",
    "        # Actions\n",
    "        self.actions = {0: Movements.up, \n",
    "                        1: Movements.down, \n",
    "                        2: Movements.left, \n",
    "                        3: Movements.right}\n",
    "        \n",
    "        # Log in a list all the matrices in each step\n",
    "        self.log = defaultdict(list)\n",
    "        \n",
    "        # Initialize matrix\n",
    "        self._init_matrix(grid_size)\n",
    "        \n",
    "        # Score accumulated\n",
    "        self.game_score = 0\n",
    "    \n",
    "        \n",
    "    def _init_matrix(self, n):\n",
    "        \"\"\"\n",
    "        Initializes the game matrix\n",
    "        \"\"\"\n",
    "        self.matrix = np.zeros((n,n))\n",
    "        self._add_two(times = 2)\n",
    "        \n",
    "        # Log\n",
    "        self.log[\"mat\"].append(self.matrix)\n",
    "        self.log[\"action\"].append(-1) # randomly added action\n",
    "        self.log[\"reward\"].append(0)\n",
    "        \n",
    "    def _add_two(self, times,  choices = c.RANDOM_NUMBER_CHOICES, probs_choices = c.PROBAB_NUMBER_CHOICES):\n",
    "        \"\"\"\n",
    "        Add to the matrix randomly a 2 and 4\n",
    "        \"\"\"\n",
    "        for _ in range(times):\n",
    "            # choose only cells with a 0\n",
    "            avail_cells = list(zip(*np.where(self.matrix==0)))\n",
    "            \n",
    "            # Choose the new index of the matrix\n",
    "            index_sample = random.sample(avail_cells, 1)[0]\n",
    "            \n",
    "            # Start the game always with a 2\n",
    "            if self.game_stat == -2:\n",
    "                self.matrix[index_sample] = 2\n",
    "                \n",
    "                # Change the stat to playing\n",
    "                self.game_stat = 0\n",
    "            \n",
    "            elif len(avail_cells):\n",
    "                # Choose randomly between a 2 or a 4 and put it in the matrix\n",
    "                value_sample = np.random.choice(choices, p = probs_choices)\n",
    "                self.matrix[index_sample] = value_sample\n",
    "            else:\n",
    "                sys.exit(\"Finished game!\")\n",
    "                \n",
    "    def _check_possible_action(self):\n",
    "        \"\"\"\n",
    "         # Check if there is any possible action to take\n",
    "         without modifying the env matrix\n",
    "        \"\"\"\n",
    "        any_action_available = False\n",
    "        test_matrix = copy.copy(self.matrix)\n",
    "        for a_id in self.actions:\n",
    "            _, action_available, _, _ = self.actions[a_id](test_matrix, 0)\n",
    "            any_action_available |= action_available\n",
    "        return any_action_available\n",
    "        \n",
    "        \n",
    "                \n",
    "    def _game_stat(self):\n",
    "        \"\"\"\n",
    "        Status of the game:\n",
    "        1: game won\n",
    "        -1: game lost\n",
    "        0: game in play\n",
    "        \"\"\"            \n",
    "        if self.matrix.min() == 0:\n",
    "            if self.matrix.max() >= c.OBJECTIVE:\n",
    "                self.game_stat = 1\n",
    "            else:\n",
    "                self.game_stat = 0\n",
    "        else:\n",
    "            if self._check_possible_action():\n",
    "                self.game_stat = 0\n",
    "            else:\n",
    "                self.game_stat = -1\n",
    "                \n",
    "        \n",
    "                \n",
    "    # Play step\n",
    "    def step(self, action_id):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        - start matrix\n",
    "        - final matrix\n",
    "        - has_moved: if the action taken has lead to a movement (True) or not (False)\n",
    "        - reward\n",
    "        - game stat\n",
    "        \"\"\"\n",
    "        \n",
    "        # Take the action\n",
    "        start_matrix = copy.copy(self.matrix)\n",
    "        self.matrix, done, num_merges, added_merge = self.actions[action_id](self.matrix)\n",
    "        \n",
    "        # New definition of reward counting merges and max value\n",
    "        reward = num_merges #+ np.log2(np.max(self.matrix))\n",
    "        \n",
    "        self.game_score += added_merge\n",
    "        \n",
    "        # Log\n",
    "        self.log[\"mat\"].append(self.matrix)\n",
    "        self.log[\"action\"].append(action_id)\n",
    "        self.log[\"reward\"].append(reward)\n",
    "        \n",
    "        # If the movement could be done\n",
    "        if done: \n",
    "            # Add randomly the next number in the matrix\n",
    "            self._add_two(times = 1)\n",
    "            \n",
    "            # Log\n",
    "            self.log[\"mat\"].append(self.matrix)\n",
    "            self.log[\"action\"].append(-1) #action -1 is a randomly added number\n",
    "            self.log[\"reward\"].append(0)\n",
    "            \n",
    "            # Check game status if a further action is possible\n",
    "            self._game_stat()\n",
    "        \n",
    "        # If the movement performed didn't change anything keep playing\n",
    "        return start_matrix, self.matrix, done, reward, self.game_stat\n",
    "    \n",
    "    # Reset\n",
    "    def reset(self):\n",
    "        self.__init__(self.matrix.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player():\n",
    "    \"\"\"\n",
    "    Debugging class for a random Player without policy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.steps = 0\n",
    "        self.game_stat = 0\n",
    "        self.log = defaultdict(list)\n",
    "    \n",
    "    def play_random(self):\n",
    "        \n",
    "        while self.game_stat == 0:\n",
    "            self.steps += 1\n",
    "            \n",
    "            # action random\n",
    "            action_pl = np.random.choice(list(self.env.actions.keys()))\n",
    "            \n",
    "            #choose a random action\n",
    "            start_matrix, end_matrix, done, reward, self.game_stat = self.env.step(action_pl)\n",
    "            \n",
    "            # log\n",
    "            self.log[\"mat_o\"].append(start_matrix)\n",
    "            self.log[\"action\"].append(action_pl)\n",
    "            self.log[\"reward\"].append(reward)\n",
    "            self.log[\"mat_f\"].append(end_matrix)\n",
    "            self.log[\"done\"].append(done)\n",
    "            self.log[\"game_stat\"].append(self.game_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, n_actions)\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.net1 = nn.Sequential(\n",
    "            nn.Linear(input_size, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, n_actions),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Assumes x is a tensor with the matrix raveled \n",
    "        with torch.float format (see preprocess of PolicyAgent)\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        \"\"\"\n",
    "        Initializes all weights and biases to the same quantity\n",
    "        to avoid initially getting stucked into a action value\n",
    "        when the network is just exploring and taking the same step\n",
    "        which may lead the matrix in the same corner without moving\n",
    "        until another action is sampled.\n",
    "        \"\"\"\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            #m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyAgent():\n",
    "    \"\"\"\n",
    "    Policy agent gets action probabilities from the model and samples actions from it\n",
    "    :params num_actions: number of actions to choose from the environment\n",
    "    :params model: instance of the PyTorch model (network)\n",
    "    :params state_repr: represent states by \"log2\" of the matrix or \"bin\" matrices \n",
    "    \"\"\"\n",
    "    # TODO: unify code with DQNAgent, as only action selector is differs.\n",
    "    def __init__(self, model, num_actions, state_repr = \"log2\", device=\"cpu\"):\n",
    "        self.model = model\n",
    "        self.device = device \n",
    "        self.num_actions = num_actions\n",
    "        self.state_repr = state_repr\n",
    "        \n",
    "    def _convert_log2(self, state):\n",
    "        \"\"\"\n",
    "        Converts the game matrix to a log2, replacing log2(0) by a 0\n",
    "        since no 1 is present in the matrix never\n",
    "        \"\"\"\n",
    "        state = np.log2(state)\n",
    "        state[state == -np.inf] = 0\n",
    "        return state\n",
    "    \n",
    "    def _to_binary(self, state, positions = c.BINARY_POSITIONS):\n",
    "        \"\"\"\n",
    "        Returns the binary representation of {0,1}^16 \n",
    "        of the matrix (each cell is converted to a \n",
    "        binary vector representing its binary number)\n",
    "        \"\"\"\n",
    "        states_flat = state.ravel().astype(int)\n",
    "        return (((states_flat[:,None] & (1 << np.arange(positions)))) > 0).astype(int)\n",
    "        \n",
    "    def preprocess(self, state):\n",
    "        \"\"\"\n",
    "        Given the game matrix (state) returns different \n",
    "        representations of such matrix to be input of\n",
    "        the neural network\n",
    "        \"\"\"\n",
    "        if self.state_repr == \"log2\":\n",
    "            state = self._convert_log2(state)\n",
    "            return torch.tensor(state.ravel(), dtype = torch.float)\n",
    "        elif self.state_repr == \"bin\":\n",
    "            state = self._to_binary(state)\n",
    "            return torch.tensor(state, dtype = torch.float).view(-1)\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def get_action_probs(self, state):\n",
    "        \"\"\"\n",
    "        Given a state matrix, get the probs of the last layer\n",
    "        \"\"\"\n",
    "        state = self.preprocess(state).to(self.device)\n",
    "        return F.softmax(self.model(state) ,dim=0).data.cpu().numpy()\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def __call__(self, state, epsilon = 0):\n",
    "        \"\"\"\n",
    "        Return actions from given a state\n",
    "        :param state: matrix of state\n",
    "        :param epsilon: epsilon value to choose from random action\n",
    "        :return: action index\n",
    "        \"\"\"\n",
    "        assert isinstance(state, np.ndarray)\n",
    "        # take a random choice\n",
    "        if (epsilon > 0) & (np.random.rand() < epsilon):\n",
    "            \n",
    "            return np.random.choice(self.num_actions) \n",
    "        \n",
    "        # Forward the state to get the actions probabilities\n",
    "        else:\n",
    "            \n",
    "            probs = self.get_action_probs(state)\n",
    "            return np.random.choice(self.num_actions, p=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonPolicy():\n",
    "    \"\"\"\n",
    "    Sets a decayment policy of the epsilon for the \n",
    "    agent to ignore the policy-based action\n",
    "    and perform a random action instead when training\n",
    "    First epochs should have higher epsilon to allow exploration\n",
    "    :params eps_decay: number of epochs in which epsilon goes from eps_start to eps_decay\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, eps_start, eps_decay, eps_final):\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_final = eps_final\n",
    "        self.eps_decay = eps_decay\n",
    "    \n",
    "    def get_epsilon(self, epoch):\n",
    "        return max(self.eps_final, self.eps_start  - epoch / self.eps_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExperienceEpisode= namedtuple('ExperienceEpisode', ('state', 'action', 'done', 'reward',  'game_stat', 'epsilon'))\n",
    "\n",
    "class ExperienceSource():\n",
    "    \"\"\"\n",
    "    Helps in the REINFORCE algorithm providing\n",
    "    - A continuous source of steps for 1 single episode until the buffer gets reset\n",
    "    - Inputs:\n",
    "        - env:\n",
    "        - agent: performs actions based on a policy (REINFORCE -> on-policy)\n",
    "        - epsilon_policy: instance of class EpsilonPolicy\n",
    "    Returns:\n",
    "        - reward: for each step \n",
    "        - state: state for each step in the episode\n",
    "        - action: action decided by the agent to be taken at each step\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, env, agent, epsilon_policy):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.epsilon_policy = epsilon_policy\n",
    "        \n",
    "        # Attributes\n",
    "        self.history = [] # history of ExperienceSource instances for 1 entire episode\n",
    "        self.steps = 0 # counter of actions performed\n",
    "        self.epsilon = epsilon_policy.get_epsilon(0) #starting epsilon at epoch = 0\n",
    "        self.env.reset() # Reset env\n",
    "\n",
    "    def populate_episode(self, epoch_num):\n",
    "        \n",
    "        # Play until the episode finishes\n",
    "        game_stat = 0\n",
    "        \n",
    "        while game_stat == 0:\n",
    "            \n",
    "            # Count steps\n",
    "            self.steps += 1\n",
    "                  \n",
    "            # use the agent's policy to choose next action and also input the epsilon policy\n",
    "            self.epsilon = self.epsilon_policy.get_epsilon(epoch_num)\n",
    "            action_id = self.agent(self.env.matrix, self.epsilon)\n",
    "            \n",
    "            # Take the choosen action\n",
    "            start_matrix, end_matrix, done, reward, game_stat = self.env.step(action_id)\n",
    "                \n",
    "            # fill the history of steps\n",
    "            self.history.append(ExperienceEpisode(state=start_matrix, action=action_id, \n",
    "                                                  done = done, reward=reward, game_stat=game_stat, epsilon = self.epsilon))\n",
    "            \n",
    "    def reset(self):\n",
    "        self.history = []\n",
    "        self.steps = 0\n",
    "        self.epsilon = self.epsilon_policy.get_epsilon(0)\n",
    "        self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValueCalc():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, rewards, gamma):\n",
    "        \"\"\"\n",
    "        Calculates the discounted total reward for every step\n",
    "        rewards: list of rewards for the whole episodes\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        sum_r = 0.0\n",
    "\n",
    "        # Calculate first the reward from the end of the local reward list\n",
    "        for r in reversed(rewards):\n",
    "\n",
    "            # The more far apart we are from the last step reward, the more discounted the reward\n",
    "            sum_r *= gamma\n",
    "\n",
    "            # local reward at that timestep\n",
    "            sum_r += r\n",
    "            res.append(sum_r)\n",
    "\n",
    "        # reverse again the resulting q-vals list\n",
    "        return list(reversed(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 , Game_scores_mean:  699 , Mean reward:  0.59 , Mean wins:  0.0 , Exec time epoch:  0.14 , Epsilon:  0.69\n",
      "Epoch:  2 , Game_scores_mean:  821 , Mean reward:  0.65 , Mean wins:  0.083 , Exec time epoch:  0.13 , Epsilon:  0.68\n",
      "Epoch:  3 , Game_scores_mean:  767 , Mean reward:  0.63 , Mean wins:  0.062 , Exec time epoch:  0.09 , Epsilon:  0.67\n",
      "Epoch:  4 , Game_scores_mean:  752 , Mean reward:  0.6 , Mean wins:  0.05 , Exec time epoch:  0.1 , Epsilon:  0.66\n",
      "Epoch:  5 , Game_scores_mean:  707 , Mean reward:  0.6 , Mean wins:  0.042 , Exec time epoch:  0.08 , Epsilon:  0.65\n",
      "Epoch:  6 , Game_scores_mean:  706 , Mean reward:  0.64 , Mean wins:  0.036 , Exec time epoch:  0.11 , Epsilon:  0.64\n",
      "Epoch:  7 , Game_scores_mean:  662 , Mean reward:  0.62 , Mean wins:  0.031 , Exec time epoch:  0.07 , Epsilon:  0.63\n",
      "Epoch:  8 , Game_scores_mean:  718 , Mean reward:  0.66 , Mean wins:  0.056 , Exec time epoch:  0.13 , Epsilon:  0.62\n",
      "Epoch:  9 , Game_scores_mean:  684 , Mean reward:  0.55 , Mean wins:  0.05 , Exec time epoch:  0.08 , Epsilon:  0.61\n",
      "Epoch:  10 , Game_scores_mean:  694 , Mean reward:  0.64 , Mean wins:  0.045 , Exec time epoch:  0.11 , Epsilon:  0.6\n",
      "Epoch:  11 , Game_scores_mean:  678 , Mean reward:  0.61 , Mean wins:  0.042 , Exec time epoch:  0.09 , Epsilon:  0.59\n",
      "Epoch:  12 , Game_scores_mean:  673 , Mean reward:  0.59 , Mean wins:  0.038 , Exec time epoch:  0.11 , Epsilon:  0.58\n",
      "Epoch:  13 , Game_scores_mean:  683 , Mean reward:  0.68 , Mean wins:  0.054 , Exec time epoch:  0.1 , Epsilon:  0.57\n",
      "Epoch:  14 , Game_scores_mean:  684 , Mean reward:  0.6 , Mean wins:  0.05 , Exec time epoch:  0.11 , Epsilon:  0.56\n",
      "Epoch:  15 , Game_scores_mean:  684 , Mean reward:  0.65 , Mean wins:  0.047 , Exec time epoch:  0.11 , Epsilon:  0.55\n",
      "Epoch:  16 , Game_scores_mean:  673 , Mean reward:  0.66 , Mean wins:  0.044 , Exec time epoch:  0.09 , Epsilon:  0.54\n",
      "Epoch:  17 , Game_scores_mean:  663 , Mean reward:  0.64 , Mean wins:  0.042 , Exec time epoch:  0.09 , Epsilon:  0.53\n",
      "Epoch:  18 , Game_scores_mean:  653 , Mean reward:  0.61 , Mean wins:  0.039 , Exec time epoch:  0.08 , Epsilon:  0.52\n",
      "Epoch:  19 , Game_scores_mean:  665 , Mean reward:  0.65 , Mean wins:  0.038 , Exec time epoch:  0.14 , Epsilon:  0.51\n",
      "Epoch:  20 , Game_scores_mean:  667 , Mean reward:  0.63 , Mean wins:  0.036 , Exec time epoch:  0.11 , Epsilon:  0.5\n",
      "Epoch:  21 , Game_scores_mean:  665 , Mean reward:  0.61 , Mean wins:  0.034 , Exec time epoch:  0.11 , Epsilon:  0.49\n",
      "Epoch:  22 , Game_scores_mean:  664 , Mean reward:  0.65 , Mean wins:  0.033 , Exec time epoch:  0.11 , Epsilon:  0.48\n",
      "Epoch:  23 , Game_scores_mean:  660 , Mean reward:  0.65 , Mean wins:  0.031 , Exec time epoch:  0.1 , Epsilon:  0.47\n",
      "Epoch:  24 , Game_scores_mean:  657 , Mean reward:  0.65 , Mean wins:  0.03 , Exec time epoch:  0.1 , Epsilon:  0.46\n",
      "Epoch:  25 , Game_scores_mean:  647 , Mean reward:  0.66 , Mean wins:  0.029 , Exec time epoch:  0.08 , Epsilon:  0.45\n",
      "Epoch:  26 , Game_scores_mean:  640 , Mean reward:  0.67 , Mean wins:  0.028 , Exec time epoch:  0.09 , Epsilon:  0.44\n",
      "Epoch:  27 , Game_scores_mean:  641 , Mean reward:  0.62 , Mean wins:  0.027 , Exec time epoch:  0.12 , Epsilon:  0.43\n",
      "Epoch:  28 , Game_scores_mean:  638 , Mean reward:  0.62 , Mean wins:  0.026 , Exec time epoch:  0.11 , Epsilon:  0.42\n",
      "Epoch:  29 , Game_scores_mean:  640 , Mean reward:  0.61 , Mean wins:  0.025 , Exec time epoch:  0.12 , Epsilon:  0.41\n",
      "Epoch:  30 , Game_scores_mean:  634 , Mean reward:  0.6 , Mean wins:  0.024 , Exec time epoch:  0.1 , Epsilon:  0.4\n",
      "Epoch:  31 , Game_scores_mean:  639 , Mean reward:  0.59 , Mean wins:  0.023 , Exec time epoch:  0.15 , Epsilon:  0.39\n",
      "Epoch:  32 , Game_scores_mean:  635 , Mean reward:  0.63 , Mean wins:  0.023 , Exec time epoch:  0.11 , Epsilon:  0.38\n",
      "Epoch:  33 , Game_scores_mean:  641 , Mean reward:  0.61 , Mean wins:  0.022 , Exec time epoch:  0.14 , Epsilon:  0.37\n",
      "Epoch:  34 , Game_scores_mean:  646 , Mean reward:  0.6 , Mean wins:  0.021 , Exec time epoch:  0.15 , Epsilon:  0.36\n",
      "Epoch:  35 , Game_scores_mean:  640 , Mean reward:  0.57 , Mean wins:  0.021 , Exec time epoch:  0.1 , Epsilon:  0.35\n",
      "Epoch:  36 , Game_scores_mean:  632 , Mean reward:  0.55 , Mean wins:  0.02 , Exec time epoch:  0.12 , Epsilon:  0.34\n",
      "Epoch:  37 , Game_scores_mean:  635 , Mean reward:  0.54 , Mean wins:  0.02 , Exec time epoch:  0.19 , Epsilon:  0.33\n",
      "Epoch:  38 , Game_scores_mean:  632 , Mean reward:  0.5 , Mean wins:  0.019 , Exec time epoch:  0.15 , Epsilon:  0.32\n",
      "Epoch:  39 , Game_scores_mean:  635 , Mean reward:  0.55 , Mean wins:  0.025 , Exec time epoch:  0.15 , Epsilon:  0.31\n",
      "Epoch:  40 , Game_scores_mean:  640 , Mean reward:  0.56 , Mean wins:  0.024 , Exec time epoch:  0.21 , Epsilon:  0.3\n",
      "Epoch:  41 , Game_scores_mean:  637 , Mean reward:  0.44 , Mean wins:  0.024 , Exec time epoch:  0.18 , Epsilon:  0.29\n",
      "Epoch:  42 , Game_scores_mean:  636 , Mean reward:  0.5 , Mean wins:  0.023 , Exec time epoch:  0.22 , Epsilon:  0.28\n",
      "Epoch:  43 , Game_scores_mean:  635 , Mean reward:  0.5 , Mean wins:  0.023 , Exec time epoch:  0.21 , Epsilon:  0.27\n",
      "Epoch:  44 , Game_scores_mean:  636 , Mean reward:  0.56 , Mean wins:  0.022 , Exec time epoch:  0.19 , Epsilon:  0.26\n",
      "Epoch:  45 , Game_scores_mean:  630 , Mean reward:  0.49 , Mean wins:  0.022 , Exec time epoch:  0.14 , Epsilon:  0.25\n",
      "Epoch:  46 , Game_scores_mean:  633 , Mean reward:  0.55 , Mean wins:  0.021 , Exec time epoch:  0.32 , Epsilon:  0.24\n",
      "Epoch:  47 , Game_scores_mean:  637 , Mean reward:  0.52 , Mean wins:  0.026 , Exec time epoch:  0.28 , Epsilon:  0.23\n",
      "Epoch:  48 , Game_scores_mean:  642 , Mean reward:  0.53 , Mean wins:  0.026 , Exec time epoch:  0.23 , Epsilon:  0.22\n",
      "Epoch:  49 , Game_scores_mean:  639 , Mean reward:  0.51 , Mean wins:  0.025 , Exec time epoch:  0.15 , Epsilon:  0.21\n",
      "Epoch:  50 , Game_scores_mean:  640 , Mean reward:  0.52 , Mean wins:  0.025 , Exec time epoch:  0.26 , Epsilon:  0.2\n",
      "Epoch:  51 , Game_scores_mean:  647 , Mean reward:  0.5 , Mean wins:  0.024 , Exec time epoch:  0.3 , Epsilon:  0.19\n",
      "Epoch:  52 , Game_scores_mean:  642 , Mean reward:  0.47 , Mean wins:  0.024 , Exec time epoch:  0.19 , Epsilon:  0.18\n",
      "Epoch:  53 , Game_scores_mean:  645 , Mean reward:  0.51 , Mean wins:  0.023 , Exec time epoch:  0.27 , Epsilon:  0.17\n",
      "Epoch:  54 , Game_scores_mean:  649 , Mean reward:  0.55 , Mean wins:  0.023 , Exec time epoch:  0.27 , Epsilon:  0.16\n",
      "Epoch:  55 , Game_scores_mean:  649 , Mean reward:  0.48 , Mean wins:  0.022 , Exec time epoch:  0.28 , Epsilon:  0.15\n",
      "Epoch:  56 , Game_scores_mean:  649 , Mean reward:  0.47 , Mean wins:  0.022 , Exec time epoch:  0.32 , Epsilon:  0.14\n",
      "Epoch:  57 , Game_scores_mean:  651 , Mean reward:  0.52 , Mean wins:  0.026 , Exec time epoch:  0.33 , Epsilon:  0.13\n",
      "Epoch:  58 , Game_scores_mean:  653 , Mean reward:  0.54 , Mean wins:  0.025 , Exec time epoch:  0.37 , Epsilon:  0.12\n",
      "Epoch:  59 , Game_scores_mean:  649 , Mean reward:  0.47 , Mean wins:  0.025 , Exec time epoch:  0.26 , Epsilon:  0.11\n",
      "Epoch:  60 , Game_scores_mean:  649 , Mean reward:  0.49 , Mean wins:  0.025 , Exec time epoch:  0.39 , Epsilon:  0.1\n",
      "Epoch:  61 , Game_scores_mean:  647 , Mean reward:  0.51 , Mean wins:  0.024 , Exec time epoch:  0.37 , Epsilon:  0.09\n",
      "Epoch:  62 , Game_scores_mean:  642 , Mean reward:  0.44 , Mean wins:  0.024 , Exec time epoch:  0.29 , Epsilon:  0.08\n",
      "Epoch:  63 , Game_scores_mean:  642 , Mean reward:  0.49 , Mean wins:  0.023 , Exec time epoch:  0.88 , Epsilon:  0.07\n",
      "Epoch:  64 , Game_scores_mean:  644 , Mean reward:  0.53 , Mean wins:  0.023 , Exec time epoch:  0.89 , Epsilon:  0.06\n",
      "Epoch:  65 , Game_scores_mean:  641 , Mean reward:  0.4 , Mean wins:  0.023 , Exec time epoch:  0.86 , Epsilon:  0.05\n",
      "Epoch:  66 , Game_scores_mean:  642 , Mean reward:  0.44 , Mean wins:  0.022 , Exec time epoch:  1.48 , Epsilon:  0.04\n",
      "Epoch:  67 , Game_scores_mean:  643 , Mean reward:  0.4 , Mean wins:  0.022 , Exec time epoch:  2.17 , Epsilon:  0.03\n",
      "Epoch:  68 , Game_scores_mean:  643 , Mean reward:  0.47 , Mean wins:  0.022 , Exec time epoch:  2.58 , Epsilon:  0.02\n",
      "Epoch:  69 , Game_scores_mean:  642 , Mean reward:  0.43 , Mean wins:  0.021 , Exec time epoch:  4.41 , Epsilon:  0.01\n",
      "Epoch:  70 , Game_scores_mean:  643 , Mean reward:  0.45 , Mean wins:  0.021 , Exec time epoch:  5.5 , Epsilon:  0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-386-fbcd458e725e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;31m# Generate a episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# Iterate through episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-366-b23c75835e79>\u001b[0m in \u001b[0;36mpopulate_episode\u001b[0;34m(self, epoch_num)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# Take the choosen action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mstart_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame_stat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# fill the history of steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-361-eca76e074786>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action_id)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# Take the action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mstart_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_merges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madded_merge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# New definition of reward counting merges and max value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-360-0a6bd2925f44>\u001b[0m in \u001b[0;36mright\u001b[0;34m(game, game_score)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mC\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mUC\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mUC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mrotate_game\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMovements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#double rotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mleft_game\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_merges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMovements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrotate_game\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame_score\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#apply left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mgame_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMovements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_game\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#undo the rotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-360-0a6bd2925f44>\u001b[0m in \u001b[0;36mro\u001b[0;34m(mat, cw, num)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mrot_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mrot_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrot90\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrot_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_clockise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrot_mat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mrot90\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/2048-TNu5m-4D/lib/python3.8/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mrot90\u001b[0;34m(m, k, axes)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__== \"__main__\":\n",
    "    \n",
    "    # Constants \n",
    "    c.GRID_LEN = 4\n",
    "    c.OBJECTIVE = 256\n",
    "    c.GAME_WIN_RATE = 0.8\n",
    "    c.STATE_REPR = \"bin\"\n",
    "    LEARNING_RATE = 0.01\n",
    "    GAMMA = 0.99\n",
    "    EPOCHS = 1000000\n",
    "    BATCHS = 4\n",
    "    \n",
    "    \n",
    "    # Game initialize\n",
    "    env = Env(c.GRID_LEN)\n",
    "    \n",
    "    eps = EpsilonPolicy(eps_start = 0.7, eps_decay = 100, eps_final = 0.01)\n",
    "    input_size = c.BINARY_POSITIONS * c.GRID_LEN**2 if c.STATE_REPR == \"bin\" else c.GRID_LEN**2\n",
    "    model = Model(input_size, len(env.actions))\n",
    "    agent = PolicyAgent(model = model, num_actions=len(env.actions), state_repr=c.STATE_REPR)\n",
    "    exp = ExperienceSource(env, agent, eps)\n",
    "    qv = QValueCalc()\n",
    "    \n",
    "    # Training\n",
    "    version = \"v2-log2\"\n",
    "    writer = SummaryWriter(comment=f\"-2048-{version}\", log_dir=f\"runs/{version}\")\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=LEARNING_RATE, alpha=0.99)\n",
    "\n",
    "    # Log\n",
    "    game_scores = [] # scores for each episode\n",
    "    steps_reach = [] # steps reached for each episode\n",
    "    game_wins = [] # whether 0: game lost, 1: game won\n",
    "        \n",
    "    # Counters\n",
    "    step_idx = 0\n",
    "    done_episodes = 0\n",
    "    epoch_idx = 0\n",
    "    \n",
    "    \n",
    "    ################\n",
    "    #   Epochs\n",
    "    ################\n",
    "    while epoch_idx < c.EPOCHS:\n",
    "\n",
    "        # Control\n",
    "        start_time = time.time()\n",
    "\n",
    "        # For each step in the episode, keep track also of states, actions, rewards -> qvals\n",
    "        batch_states, batch_actions, batch_rewards, batch_transf_states = [], [], [], []\n",
    "\n",
    "        ###############\n",
    "        # Batchs\n",
    "        ###############\n",
    "        # Play several games with the same policy\n",
    "        batch_episodes = 0\n",
    "\n",
    "        # For each batch\n",
    "        for batch_id in range(c.BATCHS):\n",
    "\n",
    "            # Generate a episode\n",
    "            exp.populate_episode(epoch_idx)\n",
    "\n",
    "            # Iterate through episode\n",
    "            for idx, exp_step in enumerate(exp.history):\n",
    "\n",
    "                # Ignore unfeasible moves\n",
    "                if not exp_step.done:\n",
    "                    continue\n",
    "\n",
    "                # Fill with experience data\n",
    "                batch_states.append(exp_step.state)\n",
    "                batch_transf_states.append(agent.preprocess(exp_step.state).data.numpy()) # save as numpy the transformed game matrix\n",
    "                batch_actions.append(int(exp_step.action))\n",
    "                batch_rewards.append(exp_step.reward)\n",
    "\n",
    "            # standarize and convert rewards to q values according to REINFORCE\n",
    "            st_rew = np.round((np.array(batch_rewards) - np.mean(batch_rewards)) / (np.std(batch_rewards)), 3)\n",
    "            batch_qvals = qv(st_rew, c.GAMMA)\n",
    "\n",
    "            # Get last step number\n",
    "            steps = len(exp.history)\n",
    "            steps_reach.append(steps)\n",
    "\n",
    "            # Get the final score in the episode\n",
    "            game_score_final = exp.env.game_score\n",
    "            game_scores.append(game_score_final)\n",
    "\n",
    "            # Get if the game was won (1) or not (0)\n",
    "            game_stat_final = 0 if exp.env.game_stat == -1 else 1\n",
    "            game_wins.append(game_stat_final)\n",
    "\n",
    "            # Reset the board to play another episode\n",
    "            # inside this batch (we play BATCHS episodes in this batch)\n",
    "            exp.reset()\n",
    "\n",
    "            # Inform Tensorboard\n",
    "        mean_rewards = float(np.mean(game_scores[-400:]))\n",
    "        mean_wins = np.round(float(np.mean(game_wins[-400:])) ,3)\n",
    "        writer.add_scalar(\"mean_100_scores\", mean_rewards, epoch_idx)\n",
    "        writer.add_scalar(\"game_score\", game_score_final, epoch_idx)\n",
    "        writer.add_scalar(\"steps\", steps, epoch_idx)\n",
    "        writer.add_scalar(\"mean_wins\", mean_wins, epoch_idx)\n",
    "\n",
    "        # When the problem is solved stop training\n",
    "        if (mean_wins > c.GAME_WIN_RATE) & (epoch_idx > 20):\n",
    "            break\n",
    "\n",
    "        ##############################\n",
    "        # Training neural network\n",
    "        ##############################\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Converting to tensors the matrices of each observation in the episode\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # shape: [# steps, c.GRID_LEN, c.GRID_LEN]\n",
    "        tensor_states = torch.FloatTensor(batch_transf_states)\n",
    "\n",
    "        # shape [# steps]\n",
    "        tensor_actions = torch.LongTensor(batch_actions)\n",
    "        tensor_qvals = torch.FloatTensor(batch_qvals)\n",
    "\n",
    "        # Forward to the network to get logits\n",
    "        # we will forward tensor states with the following shape\n",
    "        # [#steps, c.GRID_LEN * c.GRID_LEN]\n",
    "        logits = model(tensor_states.view(-1, input_size))\n",
    "\n",
    "        # Convert logits to log_softmax\n",
    "        log_softmax = F.log_softmax(logits, dim=1)\n",
    "\n",
    "        # From the probabilities got, mask with the actions taken\n",
    "        # log_softmax is [#steps in game, 4 (actions)] so we will\n",
    "        # convert it to [# steps, 1 (action taken)]\n",
    "        log_softmax_action = log_softmax.gather(1, tensor_actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # The loss will be the weighted sum over steps in the episode\n",
    "        # of the Q values (tensor_qvals) weighting the log(policy(s,a))\n",
    "        # which is the log_softmax_action\n",
    "        loss = -tensor_qvals * log_softmax_action\n",
    "        loss_mean = loss.mean()\n",
    "        writer.add_scalar(\"loss\", np.round(loss_mean.item(), 4), epoch_idx)\n",
    "\n",
    "        # Backpropagate\n",
    "        loss_mean.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Control\n",
    "        end_time = time.time()\n",
    "\n",
    "        if ((epoch_idx % 1) == 0) & (epoch_idx > 0):\n",
    "            print(\"Epoch: \", epoch_idx,\n",
    "                  \", Game_scores_mean: \", mean_game_scores,\n",
    "                  \", Mean reward: \", np.round(np.mean(batch_rewards), 2),\n",
    "                  \", Mean wins: \", mean_wins,\n",
    "                  \", Mean steps: \", mean_steps,\n",
    "                  \", Exec time epoch: \", round(end_time-start_time, 2),\n",
    "                  \", Epsilon: \", np.round(eps.get_epsilon(epoch_idx),3)\n",
    "                  )\n",
    "\n",
    "        # Reset the experience source and add epoch counter\n",
    "        exp.reset()\n",
    "        epoch_idx += 1\n",
    "\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
