{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Policy-Based REINFORCE method\n",
    "\n",
    "## New implementations\n",
    "\n",
    "- Change matrix values to log2 // binary representation\n",
    "- Change reward = # of merges + log2(new_max fo the matrix) and standardize them at the end of each batch\n",
    "- Change network architecture\n",
    "- Optimizer = RMSProp\n",
    "- Number of episodes 200k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import defaultdict, namedtuple\n",
    "os.chdir(\"/Users/davidamat/Documents/projects/2048/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/davidamat/Documents/projects/2048/src',\n",
       " '/Users/davidamat/Documents/projects/2048/src',\n",
       " '/Users/davidamat/Documents/Glovo/missing-items-pipeline',\n",
       " '/Users/davidamat/opt/anaconda3/lib/python38.zip',\n",
       " '/Users/davidamat/opt/anaconda3/lib/python3.8',\n",
       " '/Users/davidamat/opt/anaconda3/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/Users/davidamat/.local/share/virtualenvs/2048-TNu5m-4D/lib/python3.8/site-packages',\n",
       " '/Users/davidamat/.local/share/virtualenvs/2048-TNu5m-4D/lib/python3.8/site-packages/IPython/extensions',\n",
       " '/Users/davidamat/.ipython']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import constants as c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Movements():\n",
    "    \n",
    "    @staticmethod\n",
    "    def displacement_numbers(mat):\n",
    "        \"\"\"\n",
    "        Desplaza todos los numeros a la izquierda de la matriz\n",
    "        \"\"\"\n",
    "        \n",
    "        new = np.zeros(mat.shape) # crea una matrix de 0 de AxA (donde A es la c.GRID_LEN)\n",
    "        done = False\n",
    "        for i in range(c.GRID_LEN):\n",
    "            count = 0\n",
    "            for j in range(c.GRID_LEN):\n",
    "                # Si en esa fila hay un elemento no nulo\n",
    "                if mat[i][j] != 0:\n",
    "                    # Pone ese elemento en la posicion count (columna), donde esta vendra determinada por si\n",
    "                    # en esa fila, previamente, hemos encotrado algun otro elemento NO nulo\n",
    "                    new[i][count] = mat[i][j]\n",
    "\n",
    "                    # Solo con que modifiquemos la posicion de una de los numeros, ya contara como un movimiento: done = True\n",
    "                    if j != count:\n",
    "                        done = True\n",
    "\n",
    "                    # Suma al count una posicion ya que ya se ha movido a la izquerda de todo (columna 0) ese elemento\n",
    "                    count += 1\n",
    "        return (new, done)\n",
    "    \n",
    "    @staticmethod\n",
    "    def merge_numbers(mat, game_score = 0):\n",
    "        \"\"\"\n",
    "        Sum consecutive equal numbers (x) in a row\n",
    "        ending with the second position being 0\n",
    "        and the first position being the double of the number x\n",
    "        Return also the number of merges done in all the matrix\n",
    "        \"\"\"\n",
    "        num_merges = 0\n",
    "        for i in range(c.GRID_LEN):\n",
    "            for j in range(c.GRID_LEN-1): # la ultima columna j, no tiene una columna a su derecha (j+1)\n",
    "                celda = mat[i][j]\n",
    "                celda_derecha = mat[i][j+1]\n",
    "                if celda == celda_derecha and celda != 0: # si son iguales, y esta igualdad son numeros > 0, se suman\n",
    "                    merge_val = celda + celda_derecha\n",
    "                    mat[i][j] = merge_val\n",
    "                    game_score += merge_val #suma los puntos\n",
    "                    mat[i][j+1] = 0 # se deja la de la derecha vacía, esto hace que se necesite hacer otro displacement para llenar ese hueco\n",
    "                    # esto tambien hace que al leer la siguiente columna, se lea un 0, y no el numero que estaba\n",
    "                    num_merges += 1\n",
    "        return (mat, num_merges, game_score)\n",
    "    \n",
    "    @staticmethod\n",
    "    def perform_movement(game, game_score):\n",
    "        \"\"\"\n",
    "        1 - mover al maximo a la izquierda todos los numeros\n",
    "        2 - Sumamos los iguales dejando 0 en el segundo sumando\n",
    "        3 - Por seguridad, por si hemos dejado alguno sin desplazar a la izquierda (huecos generados por el merge),\n",
    "            se vuelve a aplicar sin importar el done o no (no importa si mueve a alguien o no ahora)\n",
    "        \"\"\"\n",
    "        game_disp, done_disp = Movements.displacement_numbers(game)\n",
    "        game_merged, num_merges, game_score = Movements.merge_numbers(game_disp, game_score)\n",
    "        game_final = Movements.displacement_numbers(game_merged)[0]\n",
    "        return (game_final, done_disp, num_merges, game_score)\n",
    "    \n",
    "    @staticmethod\n",
    "    def ro(mat, cw = True, num = 1): #cw: clockwise: True or False, #num: number of rotations\n",
    "        \"\"\"\n",
    "        Rota 90º las matrices para que las operaciones UP, DOWN y RIGHT se puedan hacer con la de LEFT\n",
    "        \"\"\"\n",
    "        param_clockise = (1,0) if cw else (0,1) #clockwise or counter-clockwise (see help(np.rot90))\n",
    "\n",
    "        # Cuantas rotaciones hacemos\n",
    "        rot_mat = mat\n",
    "        for _ in range(num):\n",
    "            rot_mat = np.rot90(np.array(rot_mat), axes = param_clockise)\n",
    "\n",
    "        return rot_mat\n",
    "\n",
    "    @staticmethod\n",
    "    def left(game, game_score = 0):\n",
    "        return Movements.perform_movement(game, game_score)\n",
    "    \n",
    "    @staticmethod\n",
    "    def down(game, game_score = 0):\n",
    "        \"\"\"\n",
    "        C - L - UC\n",
    "        \"\"\"\n",
    "        rotate_game = Movements.ro(game) #rotate clockwise\n",
    "        left_game, done, num_merges, game_score = Movements.left(rotate_game, game_score) #apply left\n",
    "        game_final = Movements.ro(left_game, cw = False) #undo the rotation\n",
    "        return game_final, done, num_merges, game_score\n",
    "    \n",
    "    @staticmethod\n",
    "    def up(game, game_score = 0):\n",
    "        \"\"\"\n",
    "        UC - L - C\n",
    "        \"\"\"\n",
    "        rotate_game = Movements.ro(game, cw = False) #rotate anti-clockwise\n",
    "        left_game, done, num_merges, game_score = Movements.left(rotate_game, game_score) #apply left\n",
    "        game_final = Movements.ro(left_game) #undo the rotation\n",
    "        return game_final, done, num_merges, game_score\n",
    "\n",
    "    @staticmethod\n",
    "    def right(game, game_score = 0):\n",
    "        \"\"\"\n",
    "        C - C - L - UC - UC\n",
    "        \"\"\"\n",
    "        rotate_game = Movements.ro(game,cw=True, num =2) #double rotation\n",
    "        left_game, done, num_merges, game_score = Movements.left(rotate_game, game_score) #apply left\n",
    "        game_final = Movements.ro(left_game, cw = False, num = 2) #undo the rotation\n",
    "        return game_final, done, num_merges, game_score\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env():\n",
    "    \n",
    "    def __init__(self, grid_size):\n",
    "        \"\"\"\n",
    "        grid_size: size of the matrix\n",
    "        \"\"\"\n",
    "        # Status\n",
    "        self.game_stat = -2 # 0: playing, 1: win, -1: lost, -2: ready to start\n",
    "        \n",
    "        # Actions\n",
    "        self.actions = {0: Movements.up, \n",
    "                        1: Movements.down, \n",
    "                        2: Movements.left, \n",
    "                        3: Movements.right}\n",
    "        \n",
    "        # Log in a list all the matrices in each step\n",
    "        self.log = defaultdict(list)\n",
    "        \n",
    "        # Initialize matrix\n",
    "        self._init_matrix(grid_size)\n",
    "        \n",
    "        # Score accumulated\n",
    "        self.game_score = 0\n",
    "    \n",
    "        \n",
    "    def _init_matrix(self, n):\n",
    "        \"\"\"\n",
    "        Initializes the game matrix\n",
    "        \"\"\"\n",
    "        self.matrix = np.zeros((n,n))\n",
    "        self._add_two(times = 2)\n",
    "        \n",
    "        # Log\n",
    "        self.log[\"mat\"].append(self.matrix)\n",
    "        self.log[\"action\"].append(-1) # randomly added action\n",
    "        self.log[\"reward\"].append(0)\n",
    "        \n",
    "    def _add_two(self, times,  choices = c.RANDOM_NUMBER_CHOICES, probs_choices = c.PROBAB_NUMBER_CHOICES):\n",
    "        \"\"\"\n",
    "        Add to the matrix randomly a 2 and 4\n",
    "        \"\"\"\n",
    "        for _ in range(times):\n",
    "            # choose only cells with a 0\n",
    "            avail_cells = list(zip(*np.where(self.matrix==0)))\n",
    "            \n",
    "            # Choose the new index of the matrix\n",
    "            index_sample = random.sample(avail_cells, 1)[0]\n",
    "            \n",
    "            # Start the game always with a 2\n",
    "            if self.game_stat == -2:\n",
    "                self.matrix[index_sample] = 2\n",
    "                \n",
    "                # Change the stat to playing\n",
    "                self.game_stat = 0\n",
    "            \n",
    "            elif len(avail_cells):\n",
    "                # Choose randomly between a 2 or a 4 and put it in the matrix\n",
    "                value_sample = np.random.choice(choices, p = probs_choices)\n",
    "                self.matrix[index_sample] = value_sample\n",
    "            else:\n",
    "                sys.exit(\"Finished game!\")\n",
    "                \n",
    "    def _check_possible_action(self):\n",
    "        \"\"\"\n",
    "         # Check if there is any possible action to take\n",
    "         without modifying the env matrix\n",
    "        \"\"\"\n",
    "        any_action_available = False\n",
    "        test_matrix = copy.copy(self.matrix)\n",
    "        for a_id in self.actions:\n",
    "            _, action_available, _, _ = self.actions[a_id](test_matrix, 0)\n",
    "            any_action_available |= action_available\n",
    "        return any_action_available\n",
    "        \n",
    "        \n",
    "                \n",
    "    def _game_stat(self):\n",
    "        \"\"\"\n",
    "        Status of the game:\n",
    "        1: game won\n",
    "        -1: game lost\n",
    "        0: game in play\n",
    "        \"\"\"            \n",
    "        if self.matrix.min() == 0:\n",
    "            if self.matrix.max() >= c.OBJECTIVE:\n",
    "                self.game_stat = 1\n",
    "            else:\n",
    "                self.game_stat = 0\n",
    "        else:\n",
    "            if self._check_possible_action():\n",
    "                self.game_stat = 0\n",
    "            else:\n",
    "                self.game_stat = -1\n",
    "                \n",
    "        \n",
    "                \n",
    "    # Play step\n",
    "    def step(self, action_id):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        - start matrix\n",
    "        - final matrix\n",
    "        - has_moved: if the action taken has lead to a movement (True) or not (False)\n",
    "        - reward\n",
    "        - game stat\n",
    "        \"\"\"\n",
    "        \n",
    "        # Take the action\n",
    "        start_matrix = copy.copy(self.matrix)\n",
    "        self.matrix, done, num_merges, added_merge = self.actions[action_id](self.matrix)\n",
    "        \n",
    "        # New definition of reward counting merges and max value\n",
    "        reward = num_merges + np.log2(np.max(self.matrix))\n",
    "        \n",
    "        self.game_score += added_merge\n",
    "        \n",
    "        # Log\n",
    "        self.log[\"mat\"].append(self.matrix)\n",
    "        self.log[\"action\"].append(action_id)\n",
    "        self.log[\"reward\"].append(reward)\n",
    "        \n",
    "        # If the movement could be done\n",
    "        if done: \n",
    "            # Add randomly the next number in the matrix\n",
    "            self._add_two(times = 1)\n",
    "            \n",
    "            # Log\n",
    "            self.log[\"mat\"].append(self.matrix)\n",
    "            self.log[\"action\"].append(-1) #action -1 is a randomly added number\n",
    "            self.log[\"reward\"].append(0)\n",
    "            \n",
    "            # Check game status if a further action is possible\n",
    "            self._game_stat()\n",
    "        \n",
    "        # If the movement performed didn't change anything keep playing\n",
    "        return start_matrix, self.matrix, done, reward, self.game_stat\n",
    "    \n",
    "    # Reset\n",
    "    def reset(self):\n",
    "        self.__init__(self.matrix.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player():\n",
    "    \"\"\"\n",
    "    Debugging class for a random Player without policy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.steps = 0\n",
    "        self.game_stat = 0\n",
    "        self.log = defaultdict(list)\n",
    "    \n",
    "    def play_random(self):\n",
    "        \n",
    "        while self.game_stat == 0:\n",
    "            self.steps += 1\n",
    "            \n",
    "            # action random\n",
    "            action_pl = np.random.choice(list(self.env.actions.keys()))\n",
    "            \n",
    "            #choose a random action\n",
    "            start_matrix, end_matrix, done, reward, self.game_stat = self.env.step(action_pl)\n",
    "            \n",
    "            # log\n",
    "            self.log[\"mat_o\"].append(start_matrix)\n",
    "            self.log[\"action\"].append(action_pl)\n",
    "            self.log[\"reward\"].append(reward)\n",
    "            self.log[\"mat_f\"].append(end_matrix)\n",
    "            self.log[\"done\"].append(done)\n",
    "            self.log[\"game_stat\"].append(self.game_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, n_actions)\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.net1 = nn.Sequential(\n",
    "            nn.Linear(input_size, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, n_actions),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Assumes x is a tensor with the matrix raveled \n",
    "        with torch.float format (see preprocess of PolicyAgent)\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        \"\"\"\n",
    "        Initializes all weights and biases to the same quantity\n",
    "        to avoid initially getting stucked into a action value\n",
    "        when the network is just exploring and taking the same step\n",
    "        which may lead the matrix in the same corner without moving\n",
    "        until another action is sampled.\n",
    "        \"\"\"\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            #m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyAgent():\n",
    "    \"\"\"\n",
    "    Policy agent gets action probabilities from the model and samples actions from it\n",
    "    :params num_actions: number of actions to choose from the environment\n",
    "    :params model: instance of the PyTorch model (network)\n",
    "    :params state_repr: represent states by \"log2\" of the matrix or \"bin\" matrices \n",
    "    \"\"\"\n",
    "    # TODO: unify code with DQNAgent, as only action selector is differs.\n",
    "    def __init__(self, model, num_actions, state_repr = \"log2\", device=\"cpu\"):\n",
    "        self.model = model\n",
    "        self.device = device \n",
    "        self.num_actions = num_actions\n",
    "        self.state_repr = state_repr\n",
    "        \n",
    "    def _convert_log2(self, state):\n",
    "        \"\"\"\n",
    "        Converts the game matrix to a log2, replacing log2(0) by a 0\n",
    "        since no 1 is present in the matrix never\n",
    "        \"\"\"\n",
    "        state = np.log2(state)\n",
    "        state[state == -np.inf] = 0\n",
    "        return state\n",
    "    \n",
    "    def _to_binary(self, state, positions = 15):\n",
    "        \"\"\"\n",
    "        Returns the binary representation of {0,1}^16 \n",
    "        of the matrix (each cell is converted to a \n",
    "        binary vector representing its binary number)\n",
    "        \"\"\"\n",
    "        states_flat = state.ravel().astype(int)\n",
    "        return (((states_flat[:,None] & (1 << np.arange(positions)))) > 0).astype(int)\n",
    "        \n",
    "    def preprocess(self, state):\n",
    "        \"\"\"\n",
    "        Given the game matrix (state) returns different \n",
    "        representations of such matrix to be input of\n",
    "        the neural network\n",
    "        \"\"\"\n",
    "        if self.state_repr == \"log2\":\n",
    "            state = self._convert_log2(state)\n",
    "            return torch.tensor(state.ravel(), dtype = torch.float)\n",
    "        elif self.state_repr == \"bin\":\n",
    "            state = self._to_binary(state)\n",
    "            return torch.tensor(state, dtype = torch.float)\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def get_action_probs(self, state):\n",
    "        \"\"\"\n",
    "        Given a state matrix, get the probs of the last layer\n",
    "        \"\"\"\n",
    "        state = self.preprocess(state).to(self.device)\n",
    "        return F.softmax(self.model(state) ,dim=0).data.cpu().numpy()\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def __call__(self, state, epsilon = 0):\n",
    "        \"\"\"\n",
    "        Return actions from given a state\n",
    "        :param state: matrix of state\n",
    "        :param epsilon: epsilon value to choose from random action\n",
    "        :return: action index\n",
    "        \"\"\"\n",
    "        assert isinstance(state, np.ndarray)\n",
    "        # take a random choice\n",
    "        if (epsilon > 0) & (np.random.rand() < epsilon):\n",
    "            \n",
    "            return np.random.choice(self.num_actions) \n",
    "        \n",
    "        # Forward the state to get the actions probabilities\n",
    "        else:\n",
    "            \n",
    "            probs = self.get_action_probs(state)\n",
    "            return np.random.choice(self.num_actions, p=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonPolicy():\n",
    "    \"\"\"\n",
    "    Sets a decayment policy of the epsilon for the \n",
    "    agent to ignore the policy-based action\n",
    "    and perform a random action instead when training\n",
    "    First epochs should have higher epsilon to allow exploration\n",
    "    :params eps_decay: number of epochs in which epsilon goes from eps_start to eps_decay\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, eps_start, eps_decay, eps_final):\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_final = eps_final\n",
    "        self.eps_decay = eps_decay\n",
    "    \n",
    "    def get_epsilon(self, epoch):\n",
    "        return max(self.eps_final, self.eps_start  - epoch / self.eps_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExperienceEpisode= namedtuple('ExperienceEpisode', ('state', 'action', 'done', 'reward',  'game_stat', 'epsilon'))\n",
    "\n",
    "class ExperienceSource():\n",
    "    \"\"\"\n",
    "    Helps in the REINFORCE algorithm providing\n",
    "    - A continuous source of steps for 1 single episode until the buffer gets reset\n",
    "    - Inputs:\n",
    "        - env:\n",
    "        - agent: performs actions based on a policy (REINFORCE -> on-policy)\n",
    "        - epsilon_policy: instance of class EpsilonPolicy\n",
    "    Returns:\n",
    "        - reward: for each step \n",
    "        - state: state for each step in the episode\n",
    "        - action: action decided by the agent to be taken at each step\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, env, agent, epsilon_policy):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.epsilon_policy = epsilon_policy\n",
    "        \n",
    "        # Attributes\n",
    "        self.history = [] # history of ExperienceSource instances for 1 entire episode\n",
    "        self.steps = 0 # counter of actions performed\n",
    "        self.epsilon = epsilon_policy.get_epsilon(0) #starting epsilon at epoch = 0\n",
    "        self.env.reset() # Reset env\n",
    "\n",
    "    def populate_episode(self, epoch_num):\n",
    "        \n",
    "        # Play until the episode finishes\n",
    "        game_stat = 0\n",
    "        \n",
    "        while game_stat == 0:\n",
    "            \n",
    "            # Count steps\n",
    "            self.steps += 1\n",
    "                  \n",
    "            # use the agent's policy to choose next action and also input the epsilon policy\n",
    "            self.epsilon = self.epsilon_policy.get_epsilon(epoch_num)\n",
    "            action_id = self.agent(self.env.matrix, self.epsilon)\n",
    "            \n",
    "            # Take the choosen action\n",
    "            start_matrix, end_matrix, done, reward, game_stat = self.env.step(action_id)\n",
    "                \n",
    "            # fill the history of steps\n",
    "            self.history.append(ExperienceEpisode(state=start_matrix, action=action_id, \n",
    "                                                  done = done, reward=reward, game_stat=game_stat, epsilon = self.epsilon))\n",
    "            \n",
    "    def reset(self):\n",
    "        self.history = []\n",
    "        self.steps = 0\n",
    "        self.epsilon = self.epsilon_policy.get_epsilon(0)\n",
    "        self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValueCalc():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, rewards, gamma):\n",
    "        \"\"\"\n",
    "        Calculates the discounted total reward for every step\n",
    "        rewards: list of rewards for the whole episodes\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        sum_r = 0.0\n",
    "\n",
    "        # Calculate first the reward from the end of the local reward list\n",
    "        for r in reversed(rewards):\n",
    "\n",
    "            # The more far apart we are from the last step reward, the more discounted the reward\n",
    "            sum_r *= gamma\n",
    "\n",
    "            # local reward at that timestep\n",
    "            sum_r += r\n",
    "            res.append(sum_r)\n",
    "\n",
    "        # reverse again the resulting q-vals list\n",
    "        return list(reversed(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 , Game_scores_mean:  1066 , Mean reward:  6.14 , Mean wins:  0.0 , Exec time epoch:  -0.0\n"
     ]
    }
   ],
   "source": [
    "if __name__== \"__main__\":\n",
    "    \n",
    "    # Constants \n",
    "    c.GRID_LEN = 4\n",
    "    c.OBJECTIVE = 256\n",
    "    c.GAME_WIN_RATE = 0.8\n",
    "    c.STATE_REPR = \"log2\"\n",
    "    LEARNING_RATE = 0.01\n",
    "    GAMMA = 0.99\n",
    "    EPOCHS = 1000000\n",
    "    BATCHS = 4\n",
    "    \n",
    "    \n",
    "    # Game initialize\n",
    "    env = Env(c.GRID_LEN)\n",
    "    model = Model(c.GRID_LEN**2, len(env.actions))\n",
    "    eps = EpsilonPolicy(eps_start = 0, eps_decay = 1, eps_final = 0)\n",
    "    agent = PolicyAgent(model = model, num_actions=len(env.actions))\n",
    "    exp = ExperienceSource(env, agent, eps)\n",
    "    qv = QValueCalc()\n",
    "    \n",
    "    # Training\n",
    "    version = \"v2-log2\"\n",
    "    writer = SummaryWriter(comment=f\"-2048-{version}\", log_dir=f\"runs/{version}\")\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=LEARNING_RATE, alpha=0.99)\n",
    "\n",
    "    # Log\n",
    "    game_scores = [] # scores for each episode\n",
    "    steps_reach = [] # steps reached for each episode\n",
    "    game_wins = [] # whether 0: game lost, 1: game won\n",
    "        \n",
    "    # Counters\n",
    "    step_idx = 0\n",
    "    done_episodes = 0\n",
    "    epoch_idx = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ################\n",
    "    #   Epochs\n",
    "    ################\n",
    "    while epoch_idx < EPOCHS:\n",
    "        \n",
    "        # Control\n",
    "        end_time = time.time()\n",
    "\n",
    "        if ((epoch_idx % 1) == 0) & (epoch_idx > 0):\n",
    "            print(\"Epoch: \", epoch_idx,\n",
    "                  \", Game_scores_mean: \", np.int_(np.mean(game_scores)),\n",
    "                  \", Mean reward: \", np.round(np.mean(batch_rewards), 2),\n",
    "                  \", Mean wins: \", mean_wins,\n",
    "                  \", Exec time epoch: \", round(end_time-start_time, 2))\n",
    "            \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # For each step in the episode, keep track also of states, actions, rewards -> qvals\n",
    "        batch_states, batch_actions, batch_rewards = [], [], []\n",
    "        \n",
    "        ###############\n",
    "        #   Batchs\n",
    "        ###############\n",
    "        # Play several games with the same policy\n",
    "        batch_episodes = 0\n",
    "        \n",
    "        # For each batch\n",
    "        for _ in range(BATCHS):\n",
    "        \n",
    "            # Generate a episode\n",
    "            exp.populate_episode(epoch_idx)\n",
    "\n",
    "\n",
    "            # Iterate through episode\n",
    "            for idx, exp_step in enumerate(exp.history):\n",
    "                \n",
    "                # Ignore unfeasible moves\n",
    "                if not exp_step.done:\n",
    "                    continue\n",
    "\n",
    "                # Fill with experience data\n",
    "                batch_states.append(exp_step.state)\n",
    "                batch_actions.append(int(exp_step.action))\n",
    "                batch_rewards.append(exp_step.reward)\n",
    "\n",
    "            # standarize and convert rewards to q values according to REINFORCE\n",
    "            #st_rew = np.round((np.array(batch_rewards) - np.mean(batch_rewards)) \\\n",
    "            #                  / (np.std(batch_rewards)),3)\n",
    "            batch_qvals = qv(batch_rewards, GAMMA)\n",
    "\n",
    "            # Get last step number\n",
    "            steps = len(exp.history)\n",
    "            steps_reach.append(steps)\n",
    "\n",
    "            # Get the final score in the episode\n",
    "            game_score_final = exp.env.game_score\n",
    "            game_scores.append(game_score_final)\n",
    "\n",
    "            # Get if the game was won (1) or not (0)\n",
    "            game_stat_final = 0 if exp.env.game_stat == -1 else 1\n",
    "            game_wins.append(game_stat_final)       \n",
    "        \n",
    "        # Inform Tensorboard\n",
    "        mean_rewards = float(np.mean(game_scores[-400:]))\n",
    "        mean_wins = np.round(float(np.mean(game_wins[-400:])),3)\n",
    "        writer.add_scalar(\"mean_100_scores\", mean_rewards, epoch_idx)\n",
    "        writer.add_scalar(\"game_score\", game_score_final, epoch_idx)\n",
    "        writer.add_scalar(\"steps\", steps, epoch_idx)\n",
    "        writer.add_scalar(\"mean_wins\", mean_wins, epoch_idx)\n",
    "        \n",
    "        # When the problem is solved stop training\n",
    "        if (mean_wins > c.GAME_WIN_RATE) & (epoch_idx > 20):\n",
    "            break\n",
    "        \n",
    "        ##############################\n",
    "        # Training neural network\n",
    "        ##############################\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Converting to tensors the matrices of each observation in the episode\n",
    "        # ----------------------------------------------------------------------\n",
    "        \n",
    "        # shape: [# steps, c.GRID_LEN, c.GRID_LEN]\n",
    "        tensor_states = torch.FloatTensor(batch_states)\n",
    "        \n",
    "        # shape [# steps]\n",
    "        tensor_actions = torch.LongTensor(batch_actions)\n",
    "        tensor_qvals = torch.FloatTensor(batch_qvals)\n",
    "        \n",
    "        # Forward to the network to get logits\n",
    "        # we will forwar tensor states with the following shape\n",
    "        # [# steps, c.GRID_LEN * c.GRID_LEN]\n",
    "        logits = model(tensor_states.view(-1, \n",
    "            torch.prod(torch.tensor(tensor_states.shape[-2:]),0).item()))\n",
    "        \n",
    "        # Convert logits to log_softmax\n",
    "        log_softmax = F.log_softmax(logits, dim=1)\n",
    "        \n",
    "        # From the probabilities got, mask with the actions taken\n",
    "        # log_softmax is [#steps in game, 4 (actions)] so we will\n",
    "        # convert it to [# steps, 1 (action taken)]\n",
    "        log_softmax_action = log_softmax.gather(1, tensor_actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # The loss will be the weighted sum over steps in the episode\n",
    "        # of the Q values (tensor_qvals) weighting the log(policy(s,a))\n",
    "        # which is the log_softmax_action\n",
    "        loss = -tensor_qvals * log_softmax_action\n",
    "        loss_mean = loss.mean()\n",
    "        writer.add_scalar(\"loss\", np.round(loss_mean.item(), 4), epoch_idx)\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss_mean.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Reset the experience source and add epoch counter\n",
    "        exp.reset()\n",
    "        epoch_idx += 1\n",
    "        \n",
    "    writer.close()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exp.populate_episode(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
