{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Policy-Based REINFORCE method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir(\"/Users/davidamat/Documents/projects/2048/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.remove('/Users/davidamat/Documents/projects/2048/src')\n",
    "sys.path.remove('/Users/davidamat/Documents/projects/2048/src')\n",
    "sys.path.insert(0, '/Users/davidamat/Documents/projects/2048')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import torch\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import src.common.constants as c\n",
    "from src.env import Env\n",
    "from src.model import Model\n",
    "from src.agent import PolicyAgent\n",
    "from src.epsilon import EpsilonPolicy\n",
    "from src.experience import ExperienceSource\n",
    "from src.common.utils import QValueCalc\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game initialize\n",
    "env = Env(c.GRID_LEN)\n",
    "eps = EpsilonPolicy(eps_start=0.8, eps_decay=200, eps_final=0.01)\n",
    "input_size = c.BINARY_POSITIONS * c.GRID_LEN**2 if c.STATE_REPR == \"bin\" else c.GRID_LEN**2\n",
    "model = Model(input_size, len(env.actions))\n",
    "agent = PolicyAgent(model=model, num_actions=len(env.actions), state_repr=c.STATE_REPR)\n",
    "exp = ExperienceSource(env, agent, eps)\n",
    "qv = QValueCalc()\n",
    "\n",
    "# Training\n",
    "version = \"REINF-v1-log2\"\n",
    "writer = SummaryWriter(comment=f\"-2048-{version}\", log_dir=f\"runs/{version}\")\n",
    "c.LEARNING_RATE = 0.\n",
    "optimizer = optim.Adam(model.parameters(), lr=c.LEARNING_RATE)\n",
    "#optimizer = optim.RMSprop(model.parameters(), lr=c.LEARNING_RATE, alpha=0.99)\n",
    "\n",
    "# Log\n",
    "game_scores = []  # scores for each episode\n",
    "steps_reach = []  # steps reached for each episode\n",
    "game_wins = []  # whether 0: game lost, 1: game won\n",
    "\n",
    "# Counters\n",
    "step_idx = 0\n",
    "done_episodes = 0\n",
    "epoch_idx = 0\n",
    "mean_wins = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities contain NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b026242cf599>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Generate a episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/2048/src/experience.py\u001b[0m in \u001b[0;36mpopulate_episode\u001b[0;34m(self, epoch_num)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# use the agent's policy to choose next action and also input the epsilon policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_epsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0maction_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# Take the choosen action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/2048-TNu5m-4D/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/2048/src/agent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, state, epsilon)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: probabilities contain NaN"
     ]
    }
   ],
   "source": [
    "################\n",
    "#   Epochs\n",
    "################\n",
    "\n",
    "while epoch_idx < c.EPOCHS:\n",
    "\n",
    "    # Control\n",
    "    start_time = time.time()\n",
    "\n",
    "    # For each step in the episode, keep track also of states, actions, rewards -> qvals\n",
    "    batch_states, batch_actions, batch_rewards, batch_transf_states = [], [], [], []\n",
    "\n",
    "    ###############\n",
    "    # Batchs\n",
    "    ###############\n",
    "    # Play several games with the same policy\n",
    "    batch_episodes = 0\n",
    "\n",
    "    # For each batch\n",
    "    for batch_id in range(c.BATCHS):\n",
    "\n",
    "        # Generate a episode\n",
    "        model.eval()\n",
    "        exp.populate_episode(epoch_idx)\n",
    "        \n",
    "\n",
    "        # Iterate through episode\n",
    "        for idx, exp_step in enumerate(exp.history):\n",
    "\n",
    "            # Ignore unfeasible moves\n",
    "            #if not exp_step.done:\n",
    "            #    continue\n",
    "\n",
    "            # Fill with experience data\n",
    "            batch_states.append(exp_step.state)\n",
    "            batch_transf_states.append(agent.preprocess(exp_step.state).data.numpy())  # save as numpy the transformed game matrix\n",
    "            batch_actions.append(int(exp_step.action))\n",
    "            batch_rewards.append(exp_step.reward)\n",
    "\n",
    "        # standarize and convert rewards to q values according to REINFORCE\n",
    "        st_rew = np.round((np.array(batch_rewards) - np.mean(batch_rewards)) / (np.std(batch_rewards)), 3)\n",
    "        batch_qvals = qv(st_rew, c.GAMMA)\n",
    "\n",
    "        # Get last step number\n",
    "        steps = len(exp.history)\n",
    "        steps_reach.append(steps)\n",
    "\n",
    "        # Get the final score in the episode\n",
    "        game_score_final = exp.env.game_score\n",
    "        game_scores.append(game_score_final)\n",
    "\n",
    "        # Get if the game was won (1) or not (0)\n",
    "        game_stat_final = 0 if exp.env.game_stat == -1 else 1\n",
    "        game_wins.append(game_stat_final)\n",
    "\n",
    "        # Reset the board to play another episode\n",
    "        # inside this batch (we play BATCHS episodes in this batch)\n",
    "        exp.reset()\n",
    "\n",
    "        # Inform Tensorboard\n",
    "    mean_game_scores = float(np.mean(game_scores[-c.BATCHS:]))\n",
    "    mean_wins = np.round(float(np.mean(game_wins[-c.BATCHS:])) ,3)\n",
    "    mean_steps = np.round(float(np.mean(steps_reach[-c.BATCHS:])) ,3)\n",
    "    writer.add_scalar(\"mean_game_scores\", mean_game_scores, epoch_idx)\n",
    "    writer.add_scalar(\"mean_wins\", mean_wins, epoch_idx)\n",
    "    writer.add_scalar(\"mean_steps\", mean_steps, epoch_idx)\n",
    "\n",
    "    # When the problem is solved stop training\n",
    "    if (mean_wins > c.GAME_WIN_RATE) & (epoch_idx > 20):\n",
    "        break\n",
    "\n",
    "    ##############################\n",
    "    # Training neural network\n",
    "    ##############################\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Converting to tensors the matrices of each observation in the episode\n",
    "    # ----------------------------------------------------------------------\n",
    "\n",
    "    # shape: [# steps, c.GRID_LEN, c.GRID_LEN]\n",
    "    tensor_states = torch.FloatTensor(batch_transf_states)\n",
    "\n",
    "    # shape [# steps]\n",
    "    tensor_actions = torch.LongTensor(batch_actions)\n",
    "    tensor_qvals = torch.FloatTensor(batch_qvals)\n",
    "\n",
    "    # Forward to the network to get logits\n",
    "    # we will forward tensor states with the following shape\n",
    "    # [#steps, c.GRID_LEN * c.GRID_LEN]\n",
    "    logits = model(tensor_states.view(-1, input_size))\n",
    "\n",
    "    # Convert logits to log_softmax\n",
    "    log_softmax = F.log_softmax(logits, dim=1)\n",
    "\n",
    "    # From the probabilities got, mask with the actions taken\n",
    "    # log_softmax is [#steps in game, 4 (actions)] so we will\n",
    "    # convert it to [# steps, 1 (action taken)]\n",
    "    log_softmax_action = log_softmax.gather(1, tensor_actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # The loss will be the weighted sum over steps in the episode\n",
    "    # of the Q values (tensor_qvals) weighting the log(policy(s,a))\n",
    "    # which is the log_softmax_action\n",
    "    loss = -tensor_qvals * log_softmax_action\n",
    "    loss_mean = loss.mean()\n",
    "    writer.add_scalar(\"loss\", np.round(loss_mean.item(), 4), epoch_idx)\n",
    "\n",
    "    # Backpropagate\n",
    "    loss_mean.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Control\n",
    "    end_time = time.time()\n",
    "\n",
    "    if ((epoch_idx % 1) == 0) & (epoch_idx > 0):\n",
    "        print(\"Epoch: \", epoch_idx,\n",
    "              \", Game_scores_mean: \", mean_game_scores,\n",
    "              \", Mean reward: \", np.round(np.mean(batch_rewards), 2),\n",
    "              \", Mean wins: \", mean_wins,\n",
    "              \", Mean steps: \", mean_steps,\n",
    "              \", Exec time epoch: \", round(end_time-start_time, 2),\n",
    "              \", Epsilon: \", np.round(eps.get_epsilon(epoch_idx),3)\n",
    "              )\n",
    "\n",
    "    # Reset the experience source and add epoch counter\n",
    "    exp.reset()\n",
    "    epoch_idx += 1\n",
    "    \n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PolicyAgent(model=model, num_actions=len(env.actions), state_repr=c.STATE_REPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 4., 0.],\n",
       "       [0., 0., 0., 2.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = agent.preprocess(env.matrix).to(\"cpu\")\n",
    "state2 = state.unsqueeze(0)\n",
    "state2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state3 = torch.cat((state2,state2), 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(env.matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fc1): Linear(in_features=240, out_features=64, bias=True)\n",
       "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act2): ReLU()\n",
       "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act3): ReLU()\n",
       "  (fc4): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0619,  0.0249, -0.0974,  0.0548,  0.0385, -0.0394,  0.0965, -0.0233,\n",
       "          0.0300, -0.1203,  0.0679, -0.0400,  0.0424,  0.0206,  0.0482, -0.0017,\n",
       "         -0.0353, -0.0653,  0.0604, -0.0110, -0.0048, -0.0184,  0.0908, -0.0718,\n",
       "          0.0397,  0.0171,  0.0490,  0.0237,  0.0588,  0.0685, -0.0597,  0.0605,\n",
       "         -0.0485,  0.1150,  0.0003,  0.1036,  0.0406,  0.0079, -0.0672,  0.0654,\n",
       "          0.0255,  0.0998,  0.0022, -0.0260,  0.0030,  0.0322, -0.0526, -0.0421,\n",
       "          0.0676,  0.0125,  0.0959,  0.0512, -0.0255, -0.1473, -0.0494,  0.0667,\n",
       "          0.0714, -0.1116,  0.0572, -0.0439, -0.1195,  0.0149,  0.0119, -0.0466]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1(state2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25419235, 0.2738103 , 0.22166967, 0.25032768]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.get_action_probs_batch(state2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2418167 , 0.25083888, 0.26148984, 0.24585463],\n",
       "       [0.2418167 , 0.25083888, 0.26148984, 0.24585463]], dtype=float32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.get_action_probs_batch(state3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = agent.get_action_probs(env.matrix)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_id = agent(env.matrix, eps.get_epsilon(400))\n",
    "action_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = torch.cat((state.unsqueeze(0),state.unsqueeze(0),state.unsqueeze(0)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = agent.preprocess(env.matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=240, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Linear(in_features=32, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 240])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0759,  0.1209,  0.0951, -0.1528]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.model(state.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2362, 0.2465, 0.2870, 0.2302],\n",
       "        [0.2362, 0.2465, 0.2870, 0.2302],\n",
       "        [0.2362, 0.2465, 0.2870, 0.2302]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(agent.model(states), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([240])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 240])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.model(state.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6578.4878, -6711.9526, -6119.0659, -5923.6694],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.model(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0.], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(agent.model(state), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_params = {}\n",
    "cc = 0\n",
    "for pp in model.parameters():\n",
    "    cc += 1\n",
    "    w_params[cc] = pp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 2.0606,  1.0165,  0.2747,  0.7981,  0.9662,  1.1002,  0.4950,  0.8974,\n",
       "         0.6916,  1.0788,  1.0809,  0.5211,  0.3244,  1.0796,  1.1076,  1.2800,\n",
       "         0.1578,  1.2693,  0.1017,  0.5033,  1.2008,  0.1457,  0.1646,  0.4053,\n",
       "         0.3129,  1.2557,  0.8618,  0.5862,  1.1870,  0.9735,  1.0659,  0.5500,\n",
       "        -0.0431,  1.2288,  0.4618,  1.1748,  1.0159,  0.8320,  0.6053,  0.4324,\n",
       "         1.3057,  0.3396,  0.6723,  0.6246,  0.9610,  0.4004,  0.9570,  0.4326,\n",
       "         0.4118,  0.7392,  0.9885,  0.2897,  0.1424,  1.7120,  0.3997,  1.2166,\n",
       "         2.4558,  1.2201,  0.8548,  1.0597,  0.7209,  1.3356,  0.9146,  0.8685,\n",
       "         1.3619,  1.0538,  1.0836,  1.0526,  0.7275,  0.7517,  0.4334,  0.2229,\n",
       "         0.4063,  0.6379,  0.5772,  0.6810,  0.5858,  0.7950,  0.2810,  0.2224,\n",
       "         0.1221,  0.8526,  0.2517,  1.0931,  0.8010,  1.4060,  0.8976,  0.1573,\n",
       "         0.0058,  1.3235,  0.5768,  0.9079,  1.2512,  0.3211,  0.6734,  0.9175,\n",
       "         0.4954,  1.0868,  0.6409,  0.5454,  0.6460,  0.4778,  1.6764,  0.3046,\n",
       "         0.5704,  1.1196,  1.2100,  0.5088,  1.3095,  0.6360,  0.3778,  1.0491,\n",
       "         1.1498,  0.9735,  0.9052,  0.6275,  0.3086,  0.6646,  0.6733,  0.3739,\n",
       "         0.7341,  1.0208,  0.1036,  0.9875,  0.7987,  0.8305,  0.5306,  0.6925,\n",
       "         1.3150,  1.0439,  0.5041,  0.9813,  0.4978,  0.3794,  0.8628,  1.4128,\n",
       "         1.2212,  1.1410,  0.8140,  1.0242,  0.6171,  1.1853,  0.3203,  0.6594,\n",
       "         0.8213,  1.0943,  0.5094,  0.6110,  1.2998,  0.8143,  0.0846,  1.1421,\n",
       "         1.0063,  1.0255,  1.0587,  0.0824,  0.6899,  1.6333,  0.5359,  0.9121,\n",
       "         1.0787,  1.0982,  0.5526,  0.0743,  0.5810, -0.1359,  0.8191,  1.1673,\n",
       "         1.2650,  0.6402,  1.2961,  0.8819,  1.1653,  1.1516,  1.2878,  0.7280,\n",
       "         0.8423,  0.8242,  0.8382,  1.1545,  0.3645,  1.2923,  0.4311,  0.3374,\n",
       "         0.5476,  0.4892,  1.2482,  2.2053,  0.5771,  0.6729,  0.4247,  0.8544,\n",
       "         1.7427,  0.7420,  1.0138,  0.4118,  1.0576,  1.8057,  0.9063, -0.0143,\n",
       "         0.9719,  1.5388,  0.6114,  1.3804,  1.2325, -0.1617,  0.3616,  0.2896,\n",
       "         0.6195,  0.6412,  0.6388,  0.2335,  0.3941,  0.6169,  1.2444,  0.1292,\n",
       "         0.4158,  0.2671,  1.0654,  1.1160,  0.7546,  1.0709,  0.8764,  0.3954,\n",
       "         0.4790,  0.4352,  0.7081,  0.6705,  0.1363,  0.3146,  0.5999,  0.6228,\n",
       "         0.8774,  0.0557,  1.0323,  0.1553,  0.2033,  1.2766,  1.1106,  0.5280,\n",
       "         1.1074,  0.1813,  0.9491,  0.7926,  1.5594,  0.6995,  0.3913,  0.5835,\n",
       "         1.3057,  0.3940,  0.6319,  0.4906,  1.1131,  0.3164,  0.1491,  1.1578],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_params[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 240])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_params[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
