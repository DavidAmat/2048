{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import defaultdict, namedtuple\n",
    "os.chdir(\"/Users/davidamat/Documents/projects/2048/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/davidamat/Documents/projects/2048/src',\n",
       " '/Users/davidamat/Documents/projects/2048/src',\n",
       " '/Users/davidamat/Documents/Glovo/missing-items-pipeline',\n",
       " '/Users/davidamat/opt/anaconda3/lib/python38.zip',\n",
       " '/Users/davidamat/opt/anaconda3/lib/python3.8',\n",
       " '/Users/davidamat/opt/anaconda3/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/Users/davidamat/.local/share/virtualenvs/2048-TNu5m-4D/lib/python3.8/site-packages',\n",
       " '/Users/davidamat/.local/share/virtualenvs/2048-TNu5m-4D/lib/python3.8/site-packages/IPython/extensions',\n",
       " '/Users/davidamat/.ipython']"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import constants as c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Movements():\n",
    "    \n",
    "    @staticmethod\n",
    "    def displacement_numbers(mat):\n",
    "        \"\"\"\n",
    "        Desplaza todos los numeros a la izquierda de la matriz\n",
    "        \"\"\"\n",
    "        \n",
    "        new = np.zeros(mat.shape) # crea una matrix de 0 de AxA (donde A es la c.GRID_LEN)\n",
    "        done = False\n",
    "        for i in range(c.GRID_LEN):\n",
    "            count = 0\n",
    "            for j in range(c.GRID_LEN):\n",
    "                # Si en esa fila hay un elemento no nulo\n",
    "                if mat[i][j] != 0:\n",
    "                    # Pone ese elemento en la posicion count (columna), donde esta vendra determinada por si\n",
    "                    # en esa fila, previamente, hemos encotrado algun otro elemento NO nulo\n",
    "                    new[i][count] = mat[i][j]\n",
    "\n",
    "                    # Solo con que modifiquemos la posicion de una de los numeros, ya contara como un movimiento: done = True\n",
    "                    if j != count:\n",
    "                        done = True\n",
    "\n",
    "                    # Suma al count una posicion ya que ya se ha movido a la izquerda de todo (columna 0) ese elemento\n",
    "                    count += 1\n",
    "        return (new, done)\n",
    "    \n",
    "    @staticmethod\n",
    "    def merge_numbers(mat, game_score = 0):\n",
    "        \"\"\"\n",
    "        Suma los números de cada fila que sean iguales \n",
    "        y consecutivos, dejando el segundo sumando a 0\n",
    "        \"\"\"\n",
    "        done = False\n",
    "        for i in range(c.GRID_LEN):\n",
    "            for j in range(c.GRID_LEN-1): # la ultima columna j, no tiene una columna a su derecha (j+1)\n",
    "                celda = mat[i][j]\n",
    "                celda_derecha = mat[i][j+1]\n",
    "                if celda == celda_derecha and celda != 0: # si son iguales, y esta igualdad son numeros > 0, se suman\n",
    "                    merge_val = celda + celda_derecha\n",
    "                    mat[i][j] = merge_val\n",
    "                    game_score += merge_val #suma los puntos\n",
    "                    mat[i][j+1] = 0 # se deja la de la derecha vacía, esto hace que se necesite hacer otro displacement para llenar ese hueco\n",
    "                    # esto tambien hace que al leer la siguiente columna, se lea un 0, y no el numero que estaba\n",
    "                    done = True\n",
    "        return (mat, done, game_score)\n",
    "    \n",
    "    @staticmethod\n",
    "    def perform_movement(game, game_score):\n",
    "        \"\"\"\n",
    "        1 - mover al maximo a la izquierda todos los numeros\n",
    "        2 - Sumamos los iguales dejando 0 en el segundo sumando\n",
    "        3 - Por seguridad, por si hemos dejado alguno sin desplazar a la izquierda (huecos generados por el merge),\n",
    "            se vuelve a aplicar sin importar el done o no (no importa si mueve a alguien o no ahora)\n",
    "        \"\"\"\n",
    "        game_disp, done_disp = Movements.displacement_numbers(game)\n",
    "        game_merged, done_merge, game_score = Movements.merge_numbers(game_disp, game_score)\n",
    "        game_final = Movements.displacement_numbers(game_merged)[0]\n",
    "        return (game_final, done_disp or done_merge, game_score)\n",
    "    \n",
    "    @staticmethod\n",
    "    def ro(mat, cw = True, num = 1): #cw: clockwise: True or False, #num: number of rotations\n",
    "        \"\"\"\n",
    "        Rota 90º las matrices para que las operaciones UP, DOWN y RIGHT se puedan hacer con la de LEFT\n",
    "        \"\"\"\n",
    "        param_clockise = (1,0) if cw else (0,1) #clockwise or counter-clockwise (see help(np.rot90))\n",
    "\n",
    "        # Cuantas rotaciones hacemos\n",
    "        rot_mat = mat\n",
    "        for _ in range(num):\n",
    "            rot_mat = np.rot90(np.array(rot_mat), axes = param_clockise)\n",
    "\n",
    "        return rot_mat\n",
    "\n",
    "    @staticmethod\n",
    "    def left(game, game_score = 0):\n",
    "        return Movements.perform_movement(game, game_score)\n",
    "    \n",
    "    @staticmethod\n",
    "    def down(game, game_score = 0):\n",
    "        \"\"\"\n",
    "        C - L - UC\n",
    "        \"\"\"\n",
    "        rotate_game = Movements.ro(game) #rotate clockwise\n",
    "        left_game, done, game_score = Movements.left(rotate_game, game_score) #apply left\n",
    "        game_final = Movements.ro(left_game, cw = False) #undo the rotation\n",
    "        return game_final, done, game_score\n",
    "    \n",
    "    @staticmethod\n",
    "    def up(game, game_score = 0):\n",
    "        \"\"\"\n",
    "        UC - L - C\n",
    "        \"\"\"\n",
    "        rotate_game = Movements.ro(game, cw = False) #rotate anti-clockwise\n",
    "        left_game, done, game_score = Movements.left(rotate_game, game_score) #apply left\n",
    "        game_final = Movements.ro(left_game) #undo the rotation\n",
    "        return game_final, done, game_score\n",
    "\n",
    "    @staticmethod\n",
    "    def right(game, game_score = 0):\n",
    "        \"\"\"\n",
    "        C - C - L - UC - UC\n",
    "        \"\"\"\n",
    "        rotate_game = Movements.ro(game,cw=True, num =2) #double rotation\n",
    "        left_game, done, game_score = Movements.left(rotate_game, game_score) #apply left\n",
    "        game_final = Movements.ro(left_game, cw = False, num = 2) #undo the rotation\n",
    "        return game_final, done, game_score\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env():\n",
    "    \n",
    "    def __init__(self, grid_size):\n",
    "        \"\"\"\n",
    "        grid_size: size of the matrix\n",
    "        \"\"\"\n",
    "        # Status\n",
    "        self.game_stat = -2 # 0: playing, 1: win, -1: lost, -2: ready to start\n",
    "        \n",
    "        # Actions\n",
    "        self.actions = {0: Movements.up, \n",
    "                        1: Movements.down, \n",
    "                        2: Movements.left, \n",
    "                        3: Movements.right}\n",
    "        \n",
    "        # Log in a list all the matrices in each step\n",
    "        self.log = defaultdict(list)\n",
    "        \n",
    "        # Initialize matrix\n",
    "        self._init_matrix(grid_size)\n",
    "        \n",
    "        # Score accumulated\n",
    "        self.game_score = 0\n",
    "    \n",
    "        \n",
    "    def _init_matrix(self, n):\n",
    "        \"\"\"\n",
    "        Initializes the game matrix\n",
    "        \"\"\"\n",
    "        self.matrix = np.zeros((n,n))\n",
    "        self._add_two(times = 2)\n",
    "        \n",
    "        # Log\n",
    "        self.log[\"mat\"].append(self.matrix)\n",
    "        self.log[\"action\"].append(-1) # randomly added action\n",
    "        self.log[\"reward\"].append(0)\n",
    "        \n",
    "    def _add_two(self, times,  choices = c.RANDOM_NUMBER_CHOICES, probs_choices = c.PROBAB_NUMBER_CHOICES):\n",
    "        \"\"\"\n",
    "        Add to the matrix randomly a 2 and 4\n",
    "        \"\"\"\n",
    "        for _ in range(times):\n",
    "            # choose only cells with a 0\n",
    "            avail_cells = list(zip(*np.where(self.matrix==0)))\n",
    "            \n",
    "            # Choose the new index of the matrix\n",
    "            index_sample = random.sample(avail_cells, 1)[0]\n",
    "            \n",
    "            # Start the game always with a 2\n",
    "            if self.game_stat == -2:\n",
    "                self.matrix[index_sample] = 2\n",
    "                \n",
    "                # Change the stat to playing\n",
    "                self.game_stat = 0\n",
    "            \n",
    "            elif len(avail_cells):\n",
    "                # Choose randomly between a 2 or a 4 and put it in the matrix\n",
    "                value_sample = np.random.choice(choices, p = probs_choices)\n",
    "                self.matrix[index_sample] = value_sample\n",
    "            else:\n",
    "                sys.exit(\"Finished game!\")\n",
    "                \n",
    "    def _check_possible_action(self):\n",
    "        \"\"\"\n",
    "         # Check if there is any possible action to take\n",
    "         without modifying the env matrix\n",
    "        \"\"\"\n",
    "        any_action_available = False\n",
    "        test_matrix = copy.copy(self.matrix)\n",
    "        for a_id in self.actions:\n",
    "            _, action_available, _ = self.actions[a_id](test_matrix, 0)\n",
    "            any_action_available |= action_available\n",
    "        return any_action_available\n",
    "        \n",
    "        \n",
    "                \n",
    "    def _game_stat(self):\n",
    "        \"\"\"\n",
    "        Status of the game:\n",
    "        1: game won\n",
    "        -1: game lost\n",
    "        0: game in play\n",
    "        \"\"\"            \n",
    "        if self.matrix.min() == 0:\n",
    "            if self.matrix.max() >= c.OBJECTIVE:\n",
    "                self.game_stat = 1\n",
    "            else:\n",
    "                self.game_stat = 0\n",
    "        else:\n",
    "            if self._check_possible_action():\n",
    "                self.game_stat = 0\n",
    "            else:\n",
    "                self.game_stat = -1\n",
    "                \n",
    "        \n",
    "                \n",
    "    # Play step\n",
    "    def step(self, action_id):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        - start matrix\n",
    "        - final matrix\n",
    "        - has_moved: if the action taken has lead to a movement (True) or not (False)\n",
    "        - reward\n",
    "        - game stat\n",
    "        \"\"\"\n",
    "        \n",
    "        # Take the action\n",
    "        start_matrix = copy.copy(self.matrix)\n",
    "        self.matrix, done, reward = self.actions[action_id](self.matrix)\n",
    "        self.game_score += reward\n",
    "        \n",
    "        # Log\n",
    "        self.log[\"mat\"].append(self.matrix)\n",
    "        self.log[\"action\"].append(action_id)\n",
    "        self.log[\"reward\"].append(reward)\n",
    "        \n",
    "        # If the movement could be done\n",
    "        if done: \n",
    "            # Add randomly the next number in the matrix\n",
    "            self._add_two(times = 1)\n",
    "            \n",
    "            # Log\n",
    "            self.log[\"mat\"].append(self.matrix)\n",
    "            self.log[\"action\"].append(-1) #action -1 is a randomly added number\n",
    "            self.log[\"reward\"].append(0)\n",
    "            \n",
    "            # Check game status if a further action is possible\n",
    "            self._game_stat()\n",
    "        \n",
    "        # If the movement performed didn't change anything keep playing\n",
    "        return start_matrix, self.matrix, done, reward, self.game_stat\n",
    "    \n",
    "    # Reset\n",
    "    def reset(self):\n",
    "        self.__init__(self.matrix.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player():\n",
    "    \"\"\"\n",
    "    Debugging class for a random Player without policy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.steps = 0\n",
    "        self.game_stat = 0\n",
    "        self.log = defaultdict(list)\n",
    "    \n",
    "    def play_random(self):\n",
    "        \n",
    "        while self.game_stat == 0:\n",
    "            self.steps += 1\n",
    "            \n",
    "            # action random\n",
    "            action_pl = np.random.choice(list(self.env.actions.keys()))\n",
    "            \n",
    "            #choose a random action\n",
    "            start_matrix, end_matrix, done, reward, self.game_stat = self.env.step(action_pl)\n",
    "            \n",
    "            # log\n",
    "            self.log[\"mat_o\"].append(start_matrix)\n",
    "            self.log[\"action\"].append(action_pl)\n",
    "            self.log[\"reward\"].append(reward)\n",
    "            self.log[\"mat_f\"].append(end_matrix)\n",
    "            self.log[\"done\"].append(done)\n",
    "            self.log[\"game_stat\"].append(self.game_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Assumes x is a tensor with the matrix raveled \n",
    "        with torch.float format (see preprocess of PolicyAgent)\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        \"\"\"\n",
    "        Initializes all weights and biases to the same quantity\n",
    "        to avoid initially getting stucked into a action value\n",
    "        when the network is just exploring and taking the same step\n",
    "        which may lead the matrix in the same corner without moving\n",
    "        until another action is sampled.\n",
    "        \"\"\"\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            #m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyAgent():\n",
    "    \"\"\"\n",
    "    Policy agent gets action probabilities from the model and samples actions from it\n",
    "    :params num_actions: number of actions to choose from the environment\n",
    "    :params model: instance of the PyTorch model (network)\n",
    "    \"\"\"\n",
    "    # TODO: unify code with DQNAgent, as only action selector is differs.\n",
    "    def __init__(self, model, num_actions, device=\"cpu\"):\n",
    "        self.model = model\n",
    "        self.device = device \n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "    def preprocess(self, state):\n",
    "        return torch.tensor(state.ravel(), dtype = torch.float)\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def get_action_probs(self, state):\n",
    "        \"\"\"\n",
    "        Given a state matrix, get the probs of the last layer\n",
    "        \"\"\"\n",
    "        state = self.preprocess(state).to(self.device)\n",
    "        return F.softmax(self.model(state) ,dim=0).data.cpu().numpy()\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def __call__(self, state, epsilon = 0):\n",
    "        \"\"\"\n",
    "        Return actions from given a state\n",
    "        :param state: matrix of state\n",
    "        :param epsilon: epsilon value to choose from random action\n",
    "        :return: action index\n",
    "        \"\"\"\n",
    "        assert isinstance(state, np.ndarray)\n",
    "        # take a random choice\n",
    "        if (epsilon > 0) & (np.random.rand() < epsilon):\n",
    "            \n",
    "            return np.random.choice(self.num_actions) \n",
    "        \n",
    "        # Forward the state to get the actions probabilities\n",
    "        else:\n",
    "            \n",
    "            probs = self.get_action_probs(state)\n",
    "            return np.random.choice(self.num_actions, p=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonPolicy():\n",
    "    \"\"\"\n",
    "    Sets a decayment policy of the epsilon for the \n",
    "    agent to ignore the policy-based action\n",
    "    and perform a random action instead when training\n",
    "    First epochs should have higher epsilon to allow exploration\n",
    "    :params eps_decay: number of epochs in which epsilon goes from eps_start to eps_decay\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, eps_start, eps_decay, eps_final):\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_final = eps_final\n",
    "        self.eps_decay = eps_decay\n",
    "    \n",
    "    def get_epsilon(self, epoch):\n",
    "        return max(self.eps_final, self.eps_start  - epoch / self.eps_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExperienceEpisode= namedtuple('ExperienceEpisode', ('state', 'action', 'done', 'reward',  'game_stat', 'epsilon'))\n",
    "\n",
    "class ExperienceSource():\n",
    "    \"\"\"\n",
    "    Helps in the REINFORCE algorithm providing\n",
    "    - A continuous source of steps for 1 single episode until the buffer gets reset\n",
    "    - Inputs:\n",
    "        - env:\n",
    "        - agent: performs actions based on a policy (REINFORCE -> on-policy)\n",
    "        - epsilon_policy: instance of class EpsilonPolicy\n",
    "    Returns:\n",
    "        - reward: for each step \n",
    "        - state: state for each step in the episode\n",
    "        - action: action decided by the agent to be taken at each step\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, env, agent, epsilon_policy):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.epsilon_policy = epsilon_policy\n",
    "        \n",
    "        # Attributes\n",
    "        self.history = [] # history of ExperienceSource instances for 1 entire episode\n",
    "        self.steps = 0 # counter of actions performed\n",
    "        self.epsilon = epsilon_policy.get_epsilon(0) #starting epsilon at epoch = 0\n",
    "        self.env.reset() # Reset env\n",
    "\n",
    "    def populate_episode(self, epoch_num):\n",
    "        \n",
    "        # Play until the episode finishes\n",
    "        game_stat = 0\n",
    "        \n",
    "        while game_stat == 0:\n",
    "            \n",
    "            # Count steps\n",
    "            self.steps += 1\n",
    "                  \n",
    "            # use the agent's policy to choose next action and also input the epsilon policy\n",
    "            self.epsilon = self.epsilon_policy.get_epsilon(epoch_num)\n",
    "            action_id = self.agent(self.env.matrix, self.epsilon)\n",
    "            \n",
    "            # Take the choosen action\n",
    "            start_matrix, end_matrix, done, reward, game_stat = self.env.step(action_id)\n",
    "                \n",
    "            # fill the history of steps\n",
    "            self.history.append(ExperienceEpisode(state=start_matrix, action=action_id, \n",
    "                                                  done = done, reward=reward, game_stat=game_stat, epsilon = self.epsilon))\n",
    "            \n",
    "    def reset(self):\n",
    "        self.history = []\n",
    "        self.steps = 0\n",
    "        self.epsilon = self.epsilon_policy.get_epsilon(0)\n",
    "        self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValueCalc():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, rewards, gamma):\n",
    "        \"\"\"\n",
    "        Calculates the discounted total reward for every step\n",
    "        rewards: list of rewards for the whole episodes\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        sum_r = 0.0\n",
    "\n",
    "        # Calculate first the reward from the end of the local reward list\n",
    "        for r in reversed(rewards):\n",
    "\n",
    "            # The more far apart we are from the last step reward, the more discounted the reward\n",
    "            sum_r *= gamma\n",
    "\n",
    "            # local reward at that timestep\n",
    "            sum_r += r\n",
    "            res.append(sum_r)\n",
    "\n",
    "        # reverse again the resulting q-vals list\n",
    "        return list(reversed(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10 , Game_scores_mean:  844.84 , Mean wins:  0.465\n",
      "Epoch:  20 , Game_scores_mean:  832.91 , Mean wins:  0.4\n",
      "Epoch:  30 , Game_scores_mean:  744.8933333333333 , Mean wins:  0.218\n",
      "Epoch:  40 , Game_scores_mean:  762.855 , Mean wins:  0.34\n",
      "Epoch:  50 , Game_scores_mean:  787.108 , Mean wins:  0.5\n",
      "Epoch:  60 , Game_scores_mean:  783.0266666666666 , Mean wins:  0.41\n",
      "Epoch:  70 , Game_scores_mean:  784.7114285714285 , Mean wins:  0.475\n",
      "Epoch:  80 , Game_scores_mean:  792.0725 , Mean wins:  0.568\n",
      "Epoch:  90 , Game_scores_mean:  780.3911111111111 , Mean wins:  0.442\n",
      "Epoch:  100 , Game_scores_mean:  783.366 , Mean wins:  0.35\n",
      "Epoch:  110 , Game_scores_mean:  779.9781818181818 , Mean wins:  0.358\n",
      "Epoch:  120 , Game_scores_mean:  769.975 , Mean wins:  0.258\n",
      "Epoch:  130 , Game_scores_mean:  767.2738461538462 , Mean wins:  0.37\n",
      "Epoch:  140 , Game_scores_mean:  766.8142857142857 , Mean wins:  0.42\n",
      "Epoch:  150 , Game_scores_mean:  758.1146666666667 , Mean wins:  0.3\n",
      "Epoch:  160 , Game_scores_mean:  761.465 , Mean wins:  0.432\n",
      "Epoch:  170 , Game_scores_mean:  758.5741176470589 , Mean wins:  0.342\n",
      "Epoch:  180 , Game_scores_mean:  763.6711111111111 , Mean wins:  0.31\n",
      "Epoch:  190 , Game_scores_mean:  755.7305263157895 , Mean wins:  0.25\n",
      "Epoch:  200 , Game_scores_mean:  751.285 , Mean wins:  0.05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-728-fe1d0552c2f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;31m# convert rewards to q values according to REINFORCE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mbatch_qvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;31m# Get last step number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-709-d0c41a8c5775>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, rewards, gamma)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# local reward at that timestep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0msum_r\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# reverse again the resulting q-vals list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__== \"__main__\":\n",
    "    \n",
    "    # Constants \n",
    "    c.GRID_LEN = 4\n",
    "    c.OBJECTIVE = 128\n",
    "    c.GAME_WIN_RATE = 0.8\n",
    "    LEARNING_RATE = 0.001\n",
    "    GAMMA = 0.99\n",
    "    EPOCHS = 1000000\n",
    "    BATCHS = 20\n",
    "    \n",
    "    \n",
    "    # Game initialize\n",
    "    env = Env(c.GRID_LEN)\n",
    "    model = Model(c.GRID_LEN**2, len(env.actions))\n",
    "    eps = EpsilonPolicy(eps_start = 1, eps_decay = 100, eps_final = 0.01)\n",
    "    agent = PolicyAgent(model = model, num_actions=len(env.actions))\n",
    "    exp = ExperienceSource(env, agent, eps)\n",
    "    qv = QValueCalc()\n",
    "    \n",
    "    # Training\n",
    "    version = \"v1\"\n",
    "    writer = SummaryWriter(comment=f\"-2048-{version}\", log_dir=f\"runs/{version}\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Log\n",
    "    game_scores = [] # scores for each episode\n",
    "    steps_reach = [] # steps reached for each episode\n",
    "    game_wins = [] # whether 0: game lost, 1: game won\n",
    "        \n",
    "    # Counters\n",
    "    step_idx = 0\n",
    "    done_episodes = 0\n",
    "    epoch_idx = 0\n",
    "    \n",
    "    ################\n",
    "    #   Epochs\n",
    "    ################\n",
    "    while epoch_idx < EPOCHS:\n",
    "        if ((epoch_idx % 10) == 0) & (epoch_idx > 0):\n",
    "            print(\"Epoch: \", epoch_idx,\n",
    "                  \", Game_scores_mean: \" , np.mean(game_scores),\n",
    "                  \", Mean wins: \", mean_wins)\n",
    "        \n",
    "        # For each step in the episode, keep track also of states, actions, rewards -> qvals\n",
    "        batch_states, batch_actions, batch_rewards = [], [], []\n",
    "        \n",
    "        ###############\n",
    "        #   Batchs\n",
    "        ###############\n",
    "        # Play several games with the same policy\n",
    "        batch_episodes = 0\n",
    "        \n",
    "        # For each batch\n",
    "        for _ in range(BATCHS):\n",
    "        \n",
    "            # Generate a episode\n",
    "            exp.populate_episode(epoch_idx)\n",
    "\n",
    "\n",
    "            # Iterate through episode\n",
    "            for idx, exp_step in enumerate(exp.history):\n",
    "\n",
    "                # Fill with experience data\n",
    "                batch_states.append(exp_step.state)\n",
    "                batch_actions.append(int(exp_step.action))\n",
    "                batch_rewards.append(exp_step.reward)\n",
    "\n",
    "                # convert rewards to q values according to REINFORCE\n",
    "                batch_qvals = qv(batch_rewards, GAMMA)\n",
    "\n",
    "            # Get last step number\n",
    "            steps = len(exp.history)\n",
    "            steps_reach.append(steps)\n",
    "\n",
    "            # Get the final score in the episode\n",
    "            game_score_final = exp.env.game_score\n",
    "            game_scores.append(game_score_final)\n",
    "\n",
    "            # Get if the game was won (1) or not (0)\n",
    "            game_stat_final = 0 if exp.env.game_stat == -1 else 1\n",
    "            game_wins.append(game_stat_final)       \n",
    "        \n",
    "        # Inform Tensorboard\n",
    "        mean_rewards = float(np.mean(game_scores[-400:]))\n",
    "        mean_wins = np.round(float(np.mean(game_wins[-400:])),3)\n",
    "        writer.add_scalar(\"mean_100_scores\", mean_rewards, epoch_idx)\n",
    "        writer.add_scalar(\"game_score\", game_score_final, epoch_idx)\n",
    "        writer.add_scalar(\"steps\", steps, epoch_idx)\n",
    "        writer.add_scalar(\"mean_wins\", mean_wins, epoch_idx)\n",
    "        \n",
    "        # When the problem is solved stop training\n",
    "        if (mean_wins > c.GAME_WIN_RATE) & (epoch_idx > 20):\n",
    "            break\n",
    "        \n",
    "        ##############################\n",
    "        # Training neural network\n",
    "        ##############################\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Converting to tensors the matrices of each observation in the episode\n",
    "        # ----------------------------------------------------------------------\n",
    "        \n",
    "        # shape: [# steps, c.GRID_LEN, c.GRID_LEN]\n",
    "        tensor_states = torch.FloatTensor(batch_states)\n",
    "        \n",
    "        # shape [# steps]\n",
    "        tensor_actions = torch.LongTensor(batch_actions)\n",
    "        tensor_qvals = torch.FloatTensor(batch_qvals)\n",
    "        \n",
    "        # Forward to the network to get logits\n",
    "        # we will forwar tensor states with the following shape\n",
    "        # [# steps, c.GRID_LEN * c.GRID_LEN]\n",
    "        logits = model(tensor_states.view(-1, \n",
    "            torch.prod(torch.tensor(tensor_states.shape[-2:]),0).item()))\n",
    "        \n",
    "        # Convert logits to log_softmax\n",
    "        log_softmax = F.log_softmax(logits, dim=1)\n",
    "        \n",
    "        # From the probabilities got, mask with the actions taken\n",
    "        # log_softmax is [#steps in game, 4 (actions)] so we will\n",
    "        # convert it to [# steps, 1 (action taken)]\n",
    "        log_softmax_action = log_softmax.gather(1, tensor_actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # The loss will be the weighted sum over steps in the episode\n",
    "        # of the Q values (tensor_qvals) weighting the log(policy(s,a))\n",
    "        # which is the log_softmax_action\n",
    "        loss = -tensor_qvals * log_softmax_action\n",
    "        loss_mean = loss.mean()\n",
    "        writer.add_scalar(\"loss\", np.round(loss_mean.item(), 4), epoch_idx)\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss_mean.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Reset the experience source and add epoch counter\n",
    "        exp.reset()\n",
    "        epoch_idx += 1\n",
    "        \n",
    "    writer.close()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 727,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.populate_episode(15100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ExperienceEpisode(state=array([[0., 0., 2., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 2., 0.]]), action=3, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[0., 0., 0., 2.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [2., 0., 0., 0.],\n",
       "        [0., 0., 0., 2.]]), action=1, done=True, reward=4.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 2.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [2., 0., 0., 4.]]), action=1, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[2., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 2.],\n",
       "        [2., 0., 0., 4.]]), action=0, done=True, reward=4.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[4., 0., 0., 2.],\n",
       "        [2., 0., 0., 4.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]]), action=3, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[0., 0., 4., 2.],\n",
       "        [0., 0., 2., 4.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [2., 0., 0., 0.]]), action=1, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[0., 4., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 4., 2.],\n",
       "        [2., 0., 2., 4.]]), action=2, done=True, reward=4.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[4., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [4., 2., 0., 0.],\n",
       "        [4., 4., 2., 0.]]), action=1, done=True, reward=8.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 2.],\n",
       "        [4., 2., 0., 0.],\n",
       "        [8., 4., 2., 0.]]), action=2, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[0., 0., 0., 0.],\n",
       "        [2., 0., 0., 2.],\n",
       "        [4., 2., 0., 0.],\n",
       "        [8., 4., 2., 0.]]), action=0, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[2., 2., 2., 2.],\n",
       "        [4., 4., 0., 0.],\n",
       "        [8., 0., 0., 2.],\n",
       "        [0., 0., 0., 0.]]), action=2, done=True, reward=16.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[4., 4., 0., 2.],\n",
       "        [8., 0., 0., 0.],\n",
       "        [8., 2., 0., 0.],\n",
       "        [0., 0., 0., 0.]]), action=2, done=True, reward=8.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[8., 2., 0., 0.],\n",
       "        [8., 0., 0., 0.],\n",
       "        [8., 2., 2., 0.],\n",
       "        [0., 0., 0., 0.]]), action=0, done=True, reward=20.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[16.,  4.,  2.,  0.],\n",
       "        [ 8.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  2.]]), action=1, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 4.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.],\n",
       "        [16.,  0.,  0.,  0.],\n",
       "        [ 8.,  4.,  2.,  2.]]), action=0, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 4.,  4.,  2.,  2.],\n",
       "        [16.,  0.,  4.,  0.],\n",
       "        [ 8.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.]]), action=0, done=False, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 4.,  4.,  2.,  2.],\n",
       "        [16.,  0.,  4.,  0.],\n",
       "        [ 8.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.]]), action=2, done=True, reward=12.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 8.,  4.,  0.,  0.],\n",
       "        [16.,  4.,  0.,  4.],\n",
       "        [ 8.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.]]), action=3, done=True, reward=8.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  0.,  8.,  4.],\n",
       "        [ 4.,  0., 16.,  8.],\n",
       "        [ 0.,  0.,  0.,  8.],\n",
       "        [ 0.,  0.,  0.,  0.]]), action=1, done=True, reward=16.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  2.,  0.,  0.],\n",
       "        [ 0.,  0.,  8.,  4.],\n",
       "        [ 4.,  0., 16., 16.]]), action=2, done=True, reward=32.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  0.,  0.,  0.],\n",
       "        [ 2.,  0.,  0.,  4.],\n",
       "        [ 8.,  4.,  0.,  0.],\n",
       "        [ 4., 32.,  0.,  0.]]), action=3, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  2.,  4.],\n",
       "        [ 4.,  0.,  8.,  4.],\n",
       "        [ 0.,  0.,  4., 32.]]), action=3, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  2.,  2.,  4.],\n",
       "        [ 0.,  4.,  8.,  4.],\n",
       "        [ 0.,  0.,  4., 32.]]), action=1, done=True, reward=8.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  2.,  0.],\n",
       "        [ 0.,  2.,  8.,  8.],\n",
       "        [ 4.,  4.,  4., 32.]]), action=2, done=True, reward=24.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  0.,  0.,  2.],\n",
       "        [ 2.,  0.,  0.,  0.],\n",
       "        [ 2., 16.,  0.,  0.],\n",
       "        [ 8.,  4., 32.,  0.]]), action=3, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  2.,  0.,  2.],\n",
       "        [ 0.,  0.,  0.,  2.],\n",
       "        [ 0.,  0.,  2., 16.],\n",
       "        [ 0.,  8.,  4., 32.]]), action=2, done=True, reward=4.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 4.,  0.,  0.,  0.],\n",
       "        [ 2.,  2.,  0.,  0.],\n",
       "        [ 2., 16.,  0.,  0.],\n",
       "        [ 8.,  4., 32.,  0.]]), action=1, done=True, reward=4.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  0.,  0.,  0.],\n",
       "        [ 4.,  2.,  0.,  2.],\n",
       "        [ 4., 16.,  0.,  0.],\n",
       "        [ 8.,  4., 32.,  0.]]), action=1, done=True, reward=8.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  2.,  0.,  2.],\n",
       "        [ 8., 16.,  0.,  0.],\n",
       "        [ 8.,  4., 32.,  2.]]), action=3, done=True, reward=4.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  2.,  4.],\n",
       "        [ 0.,  0.,  8., 16.],\n",
       "        [ 8.,  4., 32.,  2.]]), action=2, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  0.,  0.,  0.],\n",
       "        [ 2.,  4.,  0.,  0.],\n",
       "        [ 8., 16.,  4.,  0.],\n",
       "        [ 8.,  4., 32.,  2.]]), action=2, done=False, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  0.,  0.,  0.],\n",
       "        [ 2.,  4.,  0.,  0.],\n",
       "        [ 8., 16.,  4.,  0.],\n",
       "        [ 8.,  4., 32.,  2.]]), action=2, done=False, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  0.,  0.,  0.],\n",
       "        [ 2.,  4.,  0.,  0.],\n",
       "        [ 8., 16.,  4.,  0.],\n",
       "        [ 8.,  4., 32.,  2.]]), action=0, done=True, reward=16.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 2.,  4.,  4.,  2.],\n",
       "        [16., 16., 32.,  0.],\n",
       "        [ 0.,  4.,  0.,  2.],\n",
       "        [ 0.,  0.,  0.,  0.]]), action=2, done=True, reward=40.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 2.,  8.,  2.,  0.],\n",
       "        [32., 32.,  0.,  2.],\n",
       "        [ 4.,  2.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.]]), action=2, done=True, reward=64.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 2.,  8.,  2.,  4.],\n",
       "        [64.,  2.,  0.,  0.],\n",
       "        [ 4.,  2.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.]]), action=2, done=False, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 2.,  8.,  2.,  4.],\n",
       "        [64.,  2.,  0.,  0.],\n",
       "        [ 4.,  2.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.]]), action=2, done=False, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 2.,  8.,  2.,  4.],\n",
       "        [64.,  2.,  0.,  0.],\n",
       "        [ 4.,  2.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.]]), action=1, done=True, reward=4.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  0.,  0.,  0.],\n",
       "        [ 2.,  0.,  0.,  0.],\n",
       "        [64.,  8.,  0.,  2.],\n",
       "        [ 4.,  4.,  2.,  4.]]), action=3, done=True, reward=8.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  2.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  2.],\n",
       "        [ 0., 64.,  8.,  2.],\n",
       "        [ 0.,  8.,  2.,  4.]]), action=1, done=True, reward=4.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  4.,  0.,  0.],\n",
       "        [ 0.,  2.,  0.,  0.],\n",
       "        [ 0., 64.,  8.,  4.],\n",
       "        [ 0.,  8.,  2.,  4.]]), action=1, done=True, reward=8.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  4.,  0.,  0.],\n",
       "        [ 0.,  2.,  0.,  2.],\n",
       "        [ 0., 64.,  8.,  0.],\n",
       "        [ 0.,  8.,  2.,  8.]]), action=0, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  4.,  8.,  2.],\n",
       "        [ 2.,  2.,  2.,  8.],\n",
       "        [ 0., 64.,  0.,  0.],\n",
       "        [ 0.,  8.,  0.,  0.]]), action=0, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 2.,  4.,  8.,  2.],\n",
       "        [ 0.,  2.,  2.,  8.],\n",
       "        [ 0., 64.,  0.,  0.],\n",
       "        [ 0.,  8.,  0.,  2.]]), action=3, done=True, reward=4.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 2.,  4.,  8.,  2.],\n",
       "        [ 2.,  0.,  4.,  8.],\n",
       "        [ 0.,  0.,  0., 64.],\n",
       "        [ 0.,  0.,  8.,  2.]]), action=0, done=True, reward=4.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 4.,  4.,  8.,  2.],\n",
       "        [ 0.,  0.,  4.,  8.],\n",
       "        [ 0.,  4.,  8., 64.],\n",
       "        [ 0.,  0.,  0.,  2.]]), action=0, done=True, reward=8.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 4.,  8.,  8.,  2.],\n",
       "        [ 0.,  2.,  4.,  8.],\n",
       "        [ 0.,  0.,  8., 64.],\n",
       "        [ 0.,  0.,  0.,  2.]]), action=3, done=True, reward=16.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  4., 16.,  2.],\n",
       "        [ 0.,  2.,  4.,  8.],\n",
       "        [ 0.,  0.,  8., 64.],\n",
       "        [ 2.,  0.,  0.,  2.]]), action=0, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 2.,  4., 16.,  2.],\n",
       "        [ 0.,  2.,  4.,  8.],\n",
       "        [ 0.,  2.,  8., 64.],\n",
       "        [ 0.,  0.,  0.,  2.]]), action=3, done=False, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 2.,  4., 16.,  2.],\n",
       "        [ 0.,  2.,  4.,  8.],\n",
       "        [ 0.,  2.,  8., 64.],\n",
       "        [ 0.,  0.,  0.,  2.]]), action=1, done=True, reward=4.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 2.,  0.,  0.,  2.],\n",
       "        [ 0.,  0., 16.,  8.],\n",
       "        [ 0.,  4.,  4., 64.],\n",
       "        [ 2.,  4.,  8.,  2.]]), action=2, done=True, reward=12.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 4.,  0.,  0.,  0.],\n",
       "        [16.,  8.,  4.,  0.],\n",
       "        [ 8., 64.,  0.,  0.],\n",
       "        [ 2.,  4.,  8.,  2.]]), action=2, done=False, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 4.,  0.,  0.,  0.],\n",
       "        [16.,  8.,  4.,  0.],\n",
       "        [ 8., 64.,  0.,  0.],\n",
       "        [ 2.,  4.,  8.,  2.]]), action=2, done=False, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 4.,  0.,  0.,  0.],\n",
       "        [16.,  8.,  4.,  0.],\n",
       "        [ 8., 64.,  0.,  0.],\n",
       "        [ 2.,  4.,  8.,  2.]]), action=3, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 4.,  0.,  0.,  4.],\n",
       "        [ 0., 16.,  8.,  4.],\n",
       "        [ 0.,  0.,  8., 64.],\n",
       "        [ 2.,  4.,  8.,  2.]]), action=0, done=True, reward=24.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 4., 16., 16.,  8.],\n",
       "        [ 2.,  4.,  8., 64.],\n",
       "        [ 0.,  0.,  0.,  2.],\n",
       "        [ 4.,  0.,  0.,  0.]]), action=1, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  0.,  0.,  0.],\n",
       "        [ 4.,  2.,  0.,  8.],\n",
       "        [ 2., 16., 16., 64.],\n",
       "        [ 4.,  4.,  8.,  2.]]), action=0, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 4.,  2., 16.,  8.],\n",
       "        [ 2., 16.,  8., 64.],\n",
       "        [ 4.,  4.,  4.,  2.],\n",
       "        [ 0.,  0.,  0.,  0.]]), action=3, done=True, reward=8.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 4.,  2., 16.,  8.],\n",
       "        [ 2., 16.,  8., 64.],\n",
       "        [ 2.,  4.,  8.,  2.],\n",
       "        [ 0.,  0.,  0.,  0.]]), action=1, done=True, reward=20.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  0.,  0.,  0.],\n",
       "        [ 2.,  2.,  0.,  8.],\n",
       "        [ 4., 16., 16., 64.],\n",
       "        [ 4.,  4., 16.,  2.]]), action=0, done=True, reward=40.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 2.,  2., 32.,  8.],\n",
       "        [ 8., 16.,  0., 64.],\n",
       "        [ 0.,  4.,  0.,  2.],\n",
       "        [ 0.,  2.,  0.,  0.]]), action=3, done=True, reward=4.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  4., 32.,  8.],\n",
       "        [ 0.,  8., 16., 64.],\n",
       "        [ 0.,  0.,  4.,  2.],\n",
       "        [ 0.,  0.,  2.,  2.]]), action=1, done=True, reward=4.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  2., 32.,  0.],\n",
       "        [ 0.,  0., 16.,  8.],\n",
       "        [ 0.,  4.,  4., 64.],\n",
       "        [ 0.,  8.,  2.,  4.]]), action=1, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  0., 32.,  2.],\n",
       "        [ 0.,  2., 16.,  8.],\n",
       "        [ 0.,  4.,  4., 64.],\n",
       "        [ 0.,  8.,  2.,  4.]]), action=2, done=True, reward=8.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[32.,  2.,  0.,  0.],\n",
       "        [ 2., 16.,  8.,  2.],\n",
       "        [ 8., 64.,  0.,  0.],\n",
       "        [ 8.,  2.,  4.,  0.]]), action=3, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  0., 32.,  2.],\n",
       "        [ 2., 16.,  8.,  2.],\n",
       "        [ 0.,  2.,  8., 64.],\n",
       "        [ 0.,  8.,  2.,  4.]]), action=1, done=True, reward=20.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  0.,  0.,  0.],\n",
       "        [ 4., 16., 32.,  4.],\n",
       "        [ 0.,  2., 16., 64.],\n",
       "        [ 2.,  8.,  2.,  4.]]), action=0, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 4., 16., 32.,  4.],\n",
       "        [ 2.,  2., 16., 64.],\n",
       "        [ 0.,  8.,  2.,  4.],\n",
       "        [ 0.,  0.,  2.,  0.]]), action=3, done=True, reward=4.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 4., 16., 32.,  4.],\n",
       "        [ 0.,  4., 16., 64.],\n",
       "        [ 0.,  8.,  2.,  4.],\n",
       "        [ 0.,  0.,  2.,  2.]]), action=1, done=True, reward=4.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  4.,  0.,  4.],\n",
       "        [ 0., 16., 32., 64.],\n",
       "        [ 0.,  4., 16.,  4.],\n",
       "        [ 4.,  8.,  4.,  2.]]), action=3, done=True, reward=8.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 2.,  0.,  0.,  8.],\n",
       "        [ 0., 16., 32., 64.],\n",
       "        [ 0.,  4., 16.,  4.],\n",
       "        [ 4.,  8.,  4.,  2.]]), action=3, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  0.,  2.,  8.],\n",
       "        [ 0., 16., 32., 64.],\n",
       "        [ 2.,  4., 16.,  4.],\n",
       "        [ 4.,  8.,  4.,  2.]]), action=2, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 2.,  8.,  0.,  0.],\n",
       "        [16., 32., 64.,  4.],\n",
       "        [ 2.,  4., 16.,  4.],\n",
       "        [ 4.,  8.,  4.,  2.]]), action=2, done=False, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 2.,  8.,  0.,  0.],\n",
       "        [16., 32., 64.,  4.],\n",
       "        [ 2.,  4., 16.,  4.],\n",
       "        [ 4.,  8.,  4.,  2.]]), action=0, done=True, reward=8.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 2.,  8., 64.,  8.],\n",
       "        [16., 32., 16.,  2.],\n",
       "        [ 2.,  4.,  4.,  0.],\n",
       "        [ 4.,  8.,  0.,  4.]]), action=0, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 2.,  8., 64.,  8.],\n",
       "        [16., 32., 16.,  2.],\n",
       "        [ 2.,  4.,  4.,  4.],\n",
       "        [ 4.,  8.,  2.,  0.]]), action=2, done=True, reward=8.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 2.,  8., 64.,  8.],\n",
       "        [16., 32., 16.,  2.],\n",
       "        [ 2.,  8.,  4.,  4.],\n",
       "        [ 4.,  8.,  2.,  0.]]), action=3, done=True, reward=8.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 2.,  8., 64.,  8.],\n",
       "        [16., 32., 16.,  2.],\n",
       "        [ 2.,  2.,  8.,  8.],\n",
       "        [ 0.,  4.,  8.,  2.]]), action=1, done=True, reward=16.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 0.,  8.,  2.,  8.],\n",
       "        [ 2., 32., 64.,  2.],\n",
       "        [16.,  2., 16.,  8.],\n",
       "        [ 2.,  4., 16.,  2.]]), action=2, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 8.,  2.,  8.,  2.],\n",
       "        [ 2., 32., 64.,  2.],\n",
       "        [16.,  2., 16.,  8.],\n",
       "        [ 2.,  4., 16.,  2.]]), action=3, done=False, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 8.,  2.,  8.,  2.],\n",
       "        [ 2., 32., 64.,  2.],\n",
       "        [16.,  2., 16.,  8.],\n",
       "        [ 2.,  4., 16.,  2.]]), action=0, done=True, reward=36.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 8.,  2.,  8.,  4.],\n",
       "        [ 2., 32., 64.,  8.],\n",
       "        [16.,  2., 32.,  2.],\n",
       "        [ 2.,  4.,  4.,  0.]]), action=0, done=False, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 8.,  2.,  8.,  4.],\n",
       "        [ 2., 32., 64.,  8.],\n",
       "        [16.,  2., 32.,  2.],\n",
       "        [ 2.,  4.,  4.,  0.]]), action=2, done=True, reward=8.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 8.,  2.,  8.,  4.],\n",
       "        [ 2., 32., 64.,  8.],\n",
       "        [16.,  2., 32.,  2.],\n",
       "        [ 2.,  8.,  2.,  0.]]), action=3, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 8.,  2.,  8.,  4.],\n",
       "        [ 2., 32., 64.,  8.],\n",
       "        [16.,  2., 32.,  2.],\n",
       "        [ 2.,  2.,  8.,  2.]]), action=1, done=True, reward=8.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 8.,  4.,  8.,  0.],\n",
       "        [ 2.,  2., 64.,  4.],\n",
       "        [16., 32., 32.,  8.],\n",
       "        [ 2.,  4.,  8.,  4.]]), action=2, done=True, reward=68.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 8.,  4.,  8.,  2.],\n",
       "        [ 4., 64.,  4.,  0.],\n",
       "        [16., 64.,  8.,  0.],\n",
       "        [ 2.,  4.,  8.,  4.]]), action=1, done=True, reward=144.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[  8.,   4.,   0.,   0.],\n",
       "        [  4.,   4.,   8.,   0.],\n",
       "        [ 16., 128.,   4.,   2.],\n",
       "        [  2.,   4.,  16.,   4.]]), action=2, done=True, reward=8.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[  8.,   4.,   0.,   0.],\n",
       "        [  8.,   8.,   0.,   2.],\n",
       "        [ 16., 128.,   4.,   2.],\n",
       "        [  2.,   4.,  16.,   4.]]), action=0, done=True, reward=20.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[ 16.,   4.,   4.,   4.],\n",
       "        [ 16.,   8.,  16.,   4.],\n",
       "        [  2., 128.,   0.,   0.],\n",
       "        [  0.,   4.,   2.,   0.]]), action=1, done=True, reward=40.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[  0.,   4.,   0.,   0.],\n",
       "        [  0.,   8.,   4.,   2.],\n",
       "        [ 32., 128.,  16.,   0.],\n",
       "        [  2.,   4.,   2.,   8.]]), action=1, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[  2.,   4.,   0.,   0.],\n",
       "        [  0.,   8.,   4.,   0.],\n",
       "        [ 32., 128.,  16.,   2.],\n",
       "        [  2.,   4.,   2.,   8.]]), action=1, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[  0.,   4.,   0.,   2.],\n",
       "        [  2.,   8.,   4.,   0.],\n",
       "        [ 32., 128.,  16.,   2.],\n",
       "        [  2.,   4.,   2.,   8.]]), action=0, done=True, reward=4.0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[  2.,   4.,   4.,   4.],\n",
       "        [ 32.,   8.,  16.,   8.],\n",
       "        [  2., 128.,   2.,   0.],\n",
       "        [  0.,   4.,   0.,   2.]]), action=1, done=True, reward=0, game_stat=0, epsilon=0.01),\n",
       " ExperienceEpisode(state=array([[  0.,   4.,   0.,   2.],\n",
       "        [  2.,   8.,   4.,   4.],\n",
       "        [ 32., 128.,  16.,   8.],\n",
       "        [  2.,   4.,   2.,   2.]]), action=3, done=True, reward=12.0, game_stat=0, epsilon=0.01)]"
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.history[:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
