{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Policy-Based REINFORCE method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir(\"/Users/davidamat/Documents/projects/2048/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.remove('/Users/davidamat/Documents/projects/2048/src')\n",
    "sys.path.remove('/Users/davidamat/Documents/projects/2048/src')\n",
    "sys.path.insert(0, '/Users/davidamat/Documents/projects/2048')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import torch\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import src.common.constants as c\n",
    "from src.env import Env\n",
    "from src.model import Model\n",
    "from src.agent import PolicyAgent\n",
    "from src.epsilon import EpsilonPolicy\n",
    "from src.experience import ExperienceSource\n",
    "from src.common.utils import QValueCalc\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game initialize\n",
    "env = Env(c.GRID_LEN)\n",
    "eps = EpsilonPolicy(eps_start=0.8, eps_decay=200, eps_final=0.01)\n",
    "input_size = c.BINARY_POSITIONS * c.GRID_LEN**2 if c.STATE_REPR == \"bin\" else c.GRID_LEN**2\n",
    "model = Model(input_size, len(env.actions))\n",
    "agent = PolicyAgent(model=model, num_actions=len(env.actions), state_repr=c.STATE_REPR)\n",
    "exp = ExperienceSource(env, agent, eps)\n",
    "qv = QValueCalc()\n",
    "\n",
    "# Training\n",
    "version = \"REINF-v7-log2\"\n",
    "writer = SummaryWriter(comment=f\"-2048-{version}\", log_dir=f\"runs/{version}\")\n",
    "c.LEARNING_RATE = 0.0005 \n",
    "optimizer = optim.Adam(model.parameters(), lr=c.LEARNING_RATE)\n",
    "#optimizer = optim.RMSprop(model.parameters(), lr=c.LEARNING_RATE, alpha=0.99)\n",
    "\n",
    "# Log\n",
    "game_scores = []  # scores for each episode\n",
    "steps_reach = []  # steps reached for each episode\n",
    "game_wins = []  # whether 0: game lost, 1: game won\n",
    "\n",
    "# Counters\n",
    "step_idx = 0\n",
    "done_episodes = 0\n",
    "epoch_idx = 0\n",
    "mean_wins = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 , Game_scores_mean:  1020.8 , Mean reward:  41.19 , Mean wins:  0.04 , Mean steps:  118.68 , Mean last steps:  97.415 , Epsilon:  0.795\n",
      "Epoch:  2 , Game_scores_mean:  1025.92 , Mean reward:  45.27 , Mean wins:  0.08 , Mean steps:  120.68 , Mean last steps:  85.51 , Epsilon:  0.79\n",
      "Epoch:  3 , Game_scores_mean:  850.4 , Mean reward:  36.95 , Mean wins:  0.04 , Mean steps:  107.96 , Mean last steps:  76.171 , Epsilon:  0.785\n",
      "Epoch:  4 , Game_scores_mean:  871.36 , Mean reward:  38.28 , Mean wins:  0.0 , Mean steps:  110.64 , Mean last steps:  83.209 , Epsilon:  0.78\n",
      "Epoch:  5 , Game_scores_mean:  1022.24 , Mean reward:  43.72 , Mean wins:  0.12 , Mean steps:  116.88 , Mean last steps:  90.736 , Epsilon:  0.775\n",
      "Epoch:  6 , Game_scores_mean:  1079.36 , Mean reward:  43.64 , Mean wins:  0.2 , Mean steps:  120.6 , Mean last steps:  95.706 , Epsilon:  0.77\n",
      "Epoch:  7 , Game_scores_mean:  979.36 , Mean reward:  40.56 , Mean wins:  0.08 , Mean steps:  115.52 , Mean last steps:  87.265 , Epsilon:  0.765\n",
      "Epoch:  8 , Game_scores_mean:  1072.16 , Mean reward:  44.54 , Mean wins:  0.2 , Mean steps:  118.56 , Mean last steps:  96.537 , Epsilon:  0.76\n",
      "Epoch:  9 , Game_scores_mean:  982.56 , Mean reward:  41.3 , Mean wins:  0.0 , Mean steps:  118.84 , Mean last steps:  79.313 , Epsilon:  0.755\n",
      "Epoch:  10 , Game_scores_mean:  707.2 , Mean reward:  34.03 , Mean wins:  0.0 , Mean steps:  94.56 , Mean last steps:  59.141 , Epsilon:  0.75\n",
      "Epoch:  11 , Game_scores_mean:  1027.68 , Mean reward:  45.34 , Mean wins:  0.12 , Mean steps:  117.68 , Mean last steps:  94.063 , Epsilon:  0.745\n",
      "Epoch:  12 , Game_scores_mean:  824.48 , Mean reward:  35.54 , Mean wins:  0.0 , Mean steps:  105.48 , Mean last steps:  72.15 , Epsilon:  0.74\n",
      "Epoch:  13 , Game_scores_mean:  923.2 , Mean reward:  42.97 , Mean wins:  0.08 , Mean steps:  112.4 , Mean last steps:  67.379 , Epsilon:  0.735\n",
      "Epoch:  14 , Game_scores_mean:  883.84 , Mean reward:  38.86 , Mean wins:  0.04 , Mean steps:  107.52 , Mean last steps:  86.153 , Epsilon:  0.73\n",
      "Epoch:  15 , Game_scores_mean:  1056.32 , Mean reward:  42.76 , Mean wins:  0.04 , Mean steps:  126.44 , Mean last steps:  84.551 , Epsilon:  0.725\n",
      "Epoch:  16 , Game_scores_mean:  916.48 , Mean reward:  45.86 , Mean wins:  0.08 , Mean steps:  107.76 , Mean last steps:  70.402 , Epsilon:  0.72\n",
      "Epoch:  17 , Game_scores_mean:  794.72 , Mean reward:  34.87 , Mean wins:  0.0 , Mean steps:  103.4 , Mean last steps:  65.73 , Epsilon:  0.715\n",
      "Epoch:  18 , Game_scores_mean:  932.8 , Mean reward:  39.6 , Mean wins:  0.04 , Mean steps:  114.6 , Mean last steps:  86.095 , Epsilon:  0.71\n",
      "Epoch:  19 , Game_scores_mean:  869.76 , Mean reward:  34.85 , Mean wins:  0.08 , Mean steps:  107.52 , Mean last steps:  67.7 , Epsilon:  0.705\n",
      "Epoch:  20 , Game_scores_mean:  969.92 , Mean reward:  41.6 , Mean wins:  0.08 , Mean steps:  115.68 , Mean last steps:  85.24 , Epsilon:  0.7\n",
      "Epoch:  21 , Game_scores_mean:  936.96 , Mean reward:  40.58 , Mean wins:  0.04 , Mean steps:  114.88 , Mean last steps:  101.394 , Epsilon:  0.695\n",
      "Epoch:  22 , Game_scores_mean:  902.56 , Mean reward:  35.57 , Mean wins:  0.04 , Mean steps:  110.72 , Mean last steps:  76.996 , Epsilon:  0.69\n",
      "Epoch:  23 , Game_scores_mean:  985.12 , Mean reward:  41.96 , Mean wins:  0.04 , Mean steps:  117.32 , Mean last steps:  82.892 , Epsilon:  0.685\n",
      "Epoch:  24 , Game_scores_mean:  1046.4 , Mean reward:  43.04 , Mean wins:  0.08 , Mean steps:  124.12 , Mean last steps:  86.049 , Epsilon:  0.68\n",
      "Epoch:  25 , Game_scores_mean:  961.6 , Mean reward:  43.44 , Mean wins:  0.08 , Mean steps:  114.44 , Mean last steps:  85.01 , Epsilon:  0.675\n",
      "Epoch:  26 , Game_scores_mean:  1066.08 , Mean reward:  43.05 , Mean wins:  0.12 , Mean steps:  122.2 , Mean last steps:  90.741 , Epsilon:  0.67\n",
      "Epoch:  27 , Game_scores_mean:  947.84 , Mean reward:  38.84 , Mean wins:  0.04 , Mean steps:  114.08 , Mean last steps:  87.548 , Epsilon:  0.665\n",
      "Epoch:  28 , Game_scores_mean:  918.4 , Mean reward:  39.5 , Mean wins:  0.04 , Mean steps:  112.2 , Mean last steps:  83.827 , Epsilon:  0.66\n",
      "Epoch:  29 , Game_scores_mean:  943.2 , Mean reward:  39.2 , Mean wins:  0.04 , Mean steps:  113.08 , Mean last steps:  72.195 , Epsilon:  0.655\n",
      "Epoch:  30 , Game_scores_mean:  937.28 , Mean reward:  43.74 , Mean wins:  0.0 , Mean steps:  115.28 , Mean last steps:  93.324 , Epsilon:  0.65\n",
      "Epoch:  31 , Game_scores_mean:  973.76 , Mean reward:  44.64 , Mean wins:  0.0 , Mean steps:  115.28 , Mean last steps:  92.325 , Epsilon:  0.645\n",
      "Epoch:  32 , Game_scores_mean:  877.44 , Mean reward:  35.57 , Mean wins:  0.08 , Mean steps:  109.04 , Mean last steps:  68.635 , Epsilon:  0.64\n",
      "Epoch:  33 , Game_scores_mean:  947.84 , Mean reward:  43.57 , Mean wins:  0.08 , Mean steps:  110.52 , Mean last steps:  80.14 , Epsilon:  0.635\n",
      "Epoch:  34 , Game_scores_mean:  875.36 , Mean reward:  38.58 , Mean wins:  0.0 , Mean steps:  108.6 , Mean last steps:  73.262 , Epsilon:  0.63\n",
      "Epoch:  35 , Game_scores_mean:  918.72 , Mean reward:  39.08 , Mean wins:  0.12 , Mean steps:  108.2 , Mean last steps:  77.143 , Epsilon:  0.625\n",
      "Epoch:  36 , Game_scores_mean:  1027.68 , Mean reward:  45.5 , Mean wins:  0.12 , Mean steps:  120.0 , Mean last steps:  86.177 , Epsilon:  0.62\n",
      "Epoch:  37 , Game_scores_mean:  860.0 , Mean reward:  37.9 , Mean wins:  0.0 , Mean steps:  109.96 , Mean last steps:  75.008 , Epsilon:  0.615\n",
      "Epoch:  38 , Game_scores_mean:  878.08 , Mean reward:  36.0 , Mean wins:  0.04 , Mean steps:  109.84 , Mean last steps:  70.532 , Epsilon:  0.61\n",
      "Epoch:  39 , Game_scores_mean:  895.2 , Mean reward:  37.76 , Mean wins:  0.04 , Mean steps:  109.32 , Mean last steps:  72.267 , Epsilon:  0.605\n",
      "Epoch:  40 , Game_scores_mean:  984.16 , Mean reward:  40.94 , Mean wins:  0.16 , Mean steps:  111.8 , Mean last steps:  87.849 , Epsilon:  0.6\n",
      "Epoch:  41 , Game_scores_mean:  901.28 , Mean reward:  39.37 , Mean wins:  0.0 , Mean steps:  112.16 , Mean last steps:  66.637 , Epsilon:  0.595\n",
      "Epoch:  42 , Game_scores_mean:  1129.92 , Mean reward:  45.22 , Mean wins:  0.08 , Mean steps:  128.12 , Mean last steps:  109.284 , Epsilon:  0.59\n",
      "Epoch:  43 , Game_scores_mean:  945.28 , Mean reward:  42.76 , Mean wins:  0.0 , Mean steps:  114.04 , Mean last steps:  84.943 , Epsilon:  0.585\n",
      "Epoch:  44 , Game_scores_mean:  816.32 , Mean reward:  33.71 , Mean wins:  0.08 , Mean steps:  104.6 , Mean last steps:  83.252 , Epsilon:  0.58\n",
      "Epoch:  45 , Game_scores_mean:  1021.44 , Mean reward:  45.36 , Mean wins:  0.08 , Mean steps:  118.52 , Mean last steps:  90.734 , Epsilon:  0.575\n",
      "Epoch:  46 , Game_scores_mean:  892.96 , Mean reward:  41.31 , Mean wins:  0.12 , Mean steps:  108.08 , Mean last steps:  83.886 , Epsilon:  0.57\n",
      "Epoch:  47 , Game_scores_mean:  998.88 , Mean reward:  41.51 , Mean wins:  0.12 , Mean steps:  115.12 , Mean last steps:  88.48 , Epsilon:  0.565\n",
      "Epoch:  48 , Game_scores_mean:  1064.64 , Mean reward:  45.78 , Mean wins:  0.0 , Mean steps:  124.32 , Mean last steps:  80.963 , Epsilon:  0.56\n",
      "Epoch:  49 , Game_scores_mean:  851.84 , Mean reward:  37.73 , Mean wins:  0.04 , Mean steps:  106.92 , Mean last steps:  80.524 , Epsilon:  0.555\n",
      "Epoch:  50 , Game_scores_mean:  880.64 , Mean reward:  39.52 , Mean wins:  0.04 , Mean steps:  108.92 , Mean last steps:  74.299 , Epsilon:  0.55\n",
      "Epoch:  51 , Game_scores_mean:  896.96 , Mean reward:  38.33 , Mean wins:  0.0 , Mean steps:  111.4 , Mean last steps:  69.9 , Epsilon:  0.545\n",
      "Epoch:  52 , Game_scores_mean:  789.28 , Mean reward:  31.42 , Mean wins:  0.08 , Mean steps:  101.8 , Mean last steps:  55.597 , Epsilon:  0.54\n",
      "Epoch:  53 , Game_scores_mean:  954.56 , Mean reward:  38.79 , Mean wins:  0.12 , Mean steps:  113.44 , Mean last steps:  80.917 , Epsilon:  0.535\n",
      "Epoch:  54 , Game_scores_mean:  1001.76 , Mean reward:  38.06 , Mean wins:  0.12 , Mean steps:  117.32 , Mean last steps:  79.788 , Epsilon:  0.53\n",
      "Epoch:  55 , Game_scores_mean:  945.28 , Mean reward:  43.25 , Mean wins:  0.04 , Mean steps:  115.0 , Mean last steps:  99.699 , Epsilon:  0.525\n",
      "Epoch:  56 , Game_scores_mean:  908.8 , Mean reward:  37.14 , Mean wins:  0.08 , Mean steps:  110.4 , Mean last steps:  87.851 , Epsilon:  0.52\n",
      "Epoch:  57 , Game_scores_mean:  976.8 , Mean reward:  41.45 , Mean wins:  0.04 , Mean steps:  115.72 , Mean last steps:  81.653 , Epsilon:  0.515\n",
      "Epoch:  58 , Game_scores_mean:  994.56 , Mean reward:  45.28 , Mean wins:  0.08 , Mean steps:  119.08 , Mean last steps:  98.743 , Epsilon:  0.51\n",
      "Epoch:  59 , Game_scores_mean:  1038.56 , Mean reward:  47.42 , Mean wins:  0.08 , Mean steps:  118.56 , Mean last steps:  98.402 , Epsilon:  0.505\n",
      "Epoch:  60 , Game_scores_mean:  839.2 , Mean reward:  34.83 , Mean wins:  0.04 , Mean steps:  107.12 , Mean last steps:  77.595 , Epsilon:  0.5\n",
      "Epoch:  61 , Game_scores_mean:  854.4 , Mean reward:  36.58 , Mean wins:  0.0 , Mean steps:  107.92 , Mean last steps:  81.835 , Epsilon:  0.495\n",
      "Epoch:  62 , Game_scores_mean:  1026.24 , Mean reward:  42.19 , Mean wins:  0.08 , Mean steps:  119.12 , Mean last steps:  74.01 , Epsilon:  0.49\n",
      "Epoch:  63 , Game_scores_mean:  1005.12 , Mean reward:  42.53 , Mean wins:  0.08 , Mean steps:  118.52 , Mean last steps:  87.107 , Epsilon:  0.485\n",
      "Epoch:  64 , Game_scores_mean:  923.2 , Mean reward:  42.65 , Mean wins:  0.0 , Mean steps:  112.04 , Mean last steps:  86.483 , Epsilon:  0.48\n",
      "Epoch:  65 , Game_scores_mean:  839.52 , Mean reward:  39.0 , Mean wins:  0.04 , Mean steps:  105.6 , Mean last steps:  77.791 , Epsilon:  0.475\n",
      "Epoch:  66 , Game_scores_mean:  828.0 , Mean reward:  34.81 , Mean wins:  0.0 , Mean steps:  106.64 , Mean last steps:  80.528 , Epsilon:  0.47\n",
      "Epoch:  67 , Game_scores_mean:  928.32 , Mean reward:  36.66 , Mean wins:  0.04 , Mean steps:  112.2 , Mean last steps:  79.876 , Epsilon:  0.465\n",
      "Epoch:  68 , Game_scores_mean:  872.96 , Mean reward:  36.29 , Mean wins:  0.04 , Mean steps:  110.96 , Mean last steps:  72.865 , Epsilon:  0.46\n",
      "Epoch:  69 , Game_scores_mean:  824.32 , Mean reward:  38.57 , Mean wins:  0.04 , Mean steps:  103.16 , Mean last steps:  82.249 , Epsilon:  0.455\n",
      "Epoch:  70 , Game_scores_mean:  892.16 , Mean reward:  38.93 , Mean wins:  0.0 , Mean steps:  113.32 , Mean last steps:  77.143 , Epsilon:  0.45\n",
      "Epoch:  71 , Game_scores_mean:  786.88 , Mean reward:  32.87 , Mean wins:  0.0 , Mean steps:  101.52 , Mean last steps:  66.026 , Epsilon:  0.445\n",
      "Epoch:  72 , Game_scores_mean:  990.4 , Mean reward:  43.54 , Mean wins:  0.04 , Mean steps:  116.6 , Mean last steps:  101.703 , Epsilon:  0.44\n",
      "Epoch:  73 , Game_scores_mean:  1080.96 , Mean reward:  46.75 , Mean wins:  0.04 , Mean steps:  124.24 , Mean last steps:  98.297 , Epsilon:  0.435\n",
      "Epoch:  74 , Game_scores_mean:  950.88 , Mean reward:  40.24 , Mean wins:  0.04 , Mean steps:  117.48 , Mean last steps:  69.974 , Epsilon:  0.43\n",
      "Epoch:  75 , Game_scores_mean:  893.76 , Mean reward:  38.06 , Mean wins:  0.08 , Mean steps:  107.24 , Mean last steps:  80.369 , Epsilon:  0.425\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "#   Epochs\n",
    "################\n",
    "\n",
    "while epoch_idx < c.EPOCHS:\n",
    "\n",
    "    # Control\n",
    "    start_time = time.time()\n",
    "\n",
    "    # For each step in the episode, keep track also of states, actions, rewards -> qvals\n",
    "    batch_states, batch_actions, batch_rewards, batch_transf_states = [], [], [], []\n",
    "    batch_rw_last_steps = []\n",
    "\n",
    "    ###############\n",
    "    # Batchs\n",
    "    ###############\n",
    "    # Play several games with the same policy\n",
    "    batch_episodes = 0\n",
    "\n",
    "    # For each batch\n",
    "    for batch_id in range(c.BATCHS):\n",
    "\n",
    "        # Generate a episode\n",
    "        model.eval()\n",
    "        exp.populate_episode(epoch_idx)\n",
    "        rw_last_steps = []\n",
    "\n",
    "        # Iterate through episode\n",
    "        for idx, exp_step in enumerate(exp.history):\n",
    "\n",
    "            # Ignore unfeasible moves\n",
    "            #if not exp_step.done:\n",
    "            #    continue\n",
    "\n",
    "            # Fill with experience data\n",
    "            batch_states.append(exp_step.state)\n",
    "            batch_transf_states.append(agent.preprocess(exp_step.state).data.numpy())  # save as numpy the transformed game matrix\n",
    "            batch_actions.append(int(exp_step.action))\n",
    "            batch_rewards.append(exp_step.reward)\n",
    "            rw_last_steps.append(exp_step.reward)\n",
    "\n",
    "        # standarize and convert rewards to q values according to REINFORCE\n",
    "        #st_rew = np.round((np.array(batch_rewards) - np.mean(batch_rewards)) / (np.std(batch_rewards)), 3)\n",
    "        batch_qvals = qv(np.array(batch_rewards), c.GAMMA)\n",
    "        batch_rw_last_steps.append(np.mean(rw_last_steps[-10:]))\n",
    "\n",
    "        # Get last step number\n",
    "        steps = len(exp.history)\n",
    "        steps_reach.append(steps)\n",
    "\n",
    "        # Get the final score in the episode\n",
    "        game_score_final = exp.env.game_score\n",
    "        game_scores.append(game_score_final)\n",
    "\n",
    "        # Get if the game was won (1) or not (0)\n",
    "        game_stat_final = 0 if exp.env.game_stat == -1 else 1\n",
    "        game_wins.append(game_stat_final)\n",
    "\n",
    "        # Reset the board to play another episode\n",
    "        # inside this batch (we play BATCHS episodes in this batch)\n",
    "        exp.reset()\n",
    "\n",
    "    # Inform Tensorboard\n",
    "    mean_game_scores = float(np.mean(game_scores[-c.BATCHS:]))\n",
    "    mean_wins = np.round(float(np.mean(game_wins[-c.BATCHS:])) ,3)\n",
    "    mean_steps = np.round(float(np.mean(steps_reach[-c.BATCHS:])) ,3)\n",
    "    mean_last_steps_rew = np.round(float(np.mean(batch_rw_last_steps[-c.BATCHS:])) ,3)\n",
    "    writer.add_scalar(\"mean_game_scores\", mean_game_scores, epoch_idx)\n",
    "    writer.add_scalar(\"mean_wins\", mean_wins, epoch_idx)\n",
    "    writer.add_scalar(\"mean_steps\", mean_steps, epoch_idx)\n",
    "    writer.add_scalar(\"mean_last_steps_rew\", mean_last_steps_rew, epoch_idx)\n",
    "\n",
    "    # When the problem is solved stop training\n",
    "    if (mean_wins > c.GAME_WIN_RATE) & (epoch_idx > 20):\n",
    "        break\n",
    "\n",
    "    ##############################\n",
    "    # Training neural network\n",
    "    ##############################\n",
    "    optimizer.zero_grad()\n",
    "    model.train()\n",
    "\n",
    "    # Converting to tensors the matrices of each observation in the episode\n",
    "    # ----------------------------------------------------------------------\n",
    "    # shape: [# steps, c.GRID_LEN, c.GRID_LEN]\n",
    "    tensor_states = torch.FloatTensor(batch_transf_states)\n",
    "\n",
    "    # shape [# steps]\n",
    "    tensor_actions = torch.LongTensor(batch_actions)\n",
    "    tensor_qvals = torch.FloatTensor(batch_qvals)\n",
    "\n",
    "    # Forward to the network to get logits\n",
    "    # we will forward tensor states with the following shape\n",
    "    # [#steps, c.GRID_LEN * c.GRID_LEN]\n",
    "    logits = model(tensor_states.view(-1, input_size))\n",
    "\n",
    "    # Convert logits to log_softmax\n",
    "    log_softmax = F.log_softmax(logits, dim=1)\n",
    "\n",
    "    # From the probabilities got, mask with the actions taken\n",
    "    # log_softmax is [#steps in game, 4 (actions)] so we will\n",
    "    # convert it to [# steps, 1 (action taken)]\n",
    "    log_softmax_action = log_softmax.gather(1, tensor_actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # The loss will be the weighted sum over steps in the episode\n",
    "    # of the Q values (tensor_qvals) weighting the log(policy(s,a))\n",
    "    # which is the log_softmax_action\n",
    "    loss = -tensor_qvals * log_softmax_action\n",
    "    loss_mean = loss.mean()\n",
    "    writer.add_scalar(\"loss\", np.round(loss_mean.item(), 4), epoch_idx)\n",
    "\n",
    "    # Backpropagate\n",
    "    loss_mean.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Control\n",
    "    end_time = time.time()\n",
    "\n",
    "    if ((epoch_idx % 1) == 0) & (epoch_idx > 0):\n",
    "        print(\"Epoch: \", epoch_idx,\n",
    "              \", Game_scores_mean: \", mean_game_scores,\n",
    "              \", Mean reward: \", np.round(np.mean(batch_rewards), 2),\n",
    "              \", Mean wins: \", mean_wins,\n",
    "              \", Mean steps: \", mean_steps,\n",
    "              #\", Exec time epoch: \", round(end_time-start_time, 2),\n",
    "              \", Mean last steps: \", mean_last_steps_rew,\n",
    "              \", Epsilon: \", np.round(eps.get_epsilon(epoch_idx),3)\n",
    "              )\n",
    "\n",
    "    # Reset the experience source and add epoch counter\n",
    "    exp.reset()\n",
    "    epoch_idx += 1\n",
    "    \n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env(c.GRID_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 2., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [4., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  4.,  0.],\n",
       "        [ 0.,  0.,  2.,  8.],\n",
       "        [ 4., 16., 64., 32.],\n",
       "        [ 8., 32.,  8.,  2.]]),\n",
       " array([[ 0.,  0.,  4.,  0.],\n",
       "        [ 0.,  0.,  2.,  8.],\n",
       "        [ 4., 16., 64., 32.],\n",
       "        [ 8., 32.,  8.,  2.]]),\n",
       " False,\n",
       " 0.0,\n",
       " 0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(3)\n",
    "env.step(3)\n",
    "env.step(2)\n",
    "env.step(3)\n",
    "env.step(3)\n",
    "env.step(3)\n",
    "env.step(2)\n",
    "env.step(3)\n",
    "env.step(1)\n",
    "env.step(1)\n",
    "env.step(3)\n",
    "env.step(3)\n",
    "env.step(2)\n",
    "env.step(3)\n",
    "env.step(1)\n",
    "env.step(1)\n",
    "env.step(1)\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[128.,   2.,   8.,   4.],\n",
       "       [ 64.,  32.,   2.,   0.],\n",
       "       [  4.,   8.,   0.,   0.],\n",
       "       [  0.,   2.,   0.,   0.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "bk = copy.copy(env.matrix)\n",
    "env.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <function src.common.movements.Movements.up(game, added_merge=0)>,\n",
       " 1: <function src.common.movements.Movements.down(game, added_merge=0)>,\n",
       " 2: <function src.common.movements.Movements.left(game, added_merge=0)>,\n",
       " 3: <function src.common.movements.Movements.right(game, added_merge=0)>}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  0.,   2.,   2.,   0.],\n",
       "        [128.,  32.,   0.,   0.],\n",
       "        [ 64.,   8.,   8.,   0.],\n",
       "        [  4.,   2.,   2.,   4.]]),\n",
       " array([[128.,   2.,   2.,   4.],\n",
       "        [ 64.,  32.,   8.,   0.],\n",
       "        [  4.,   8.,   2.,   0.],\n",
       "        [  2.,   2.,   0.,   0.]]),\n",
       " True,\n",
       " 128.0,\n",
       " 0)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = np.where(env.matrix == env.matrix.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.,  0.,  0., -1.],\n",
       "       [ 0.,  1.,  1.,  0.],\n",
       "       [ 0.,  1.,  1.,  0.],\n",
       "       [-1.,  0.,  0., -1.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.mask_position_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-128.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._calc_penalty_max_position(m1, env.mask_position_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._calc_penalty_max_position(m2[0], env.mask_position_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [xx[0] for xx in pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.333333333333332"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*2*2 / (6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.mask_position_val = np.ones((c.GRID_LEN,c.GRID_LEN))\n",
    "for pp in [-1,0]:\n",
    "    env.mask_position_val[:,pp] -= 1\n",
    "    env.mask_position_val[pp,:] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.,  0.,  0., -1.],\n",
       "       [ 0.,  1.,  1.,  0.],\n",
       "       [ 0.,  1.,  1.,  0.],\n",
       "       [-1.,  0.,  0., -1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.mask_position_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1944.0,\n",
       " 684.0,\n",
       " 1008.0,\n",
       " 896.0,\n",
       " 652.0,\n",
       " 576.0,\n",
       " 532.0,\n",
       " 680.0,\n",
       " 536.0,\n",
       " 940.0,\n",
       " 1064.0,\n",
       " 1020.0,\n",
       " 908.0,\n",
       " 428.0,\n",
       " 428.0,\n",
       " 644.0,\n",
       " 608.0,\n",
       " 992.0,\n",
       " 1512.0,\n",
       " 676.0,\n",
       " 1088.0,\n",
       " 1872.0,\n",
       " 1656.0,\n",
       " 1048.0,\n",
       " 320.0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_scores[-c.BATCHS:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_rw_last_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_last_steps_rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env(c.GRID_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <function src.common.movements.Movements.up(game, added_merge=0)>,\n",
       " 1: <function src.common.movements.Movements.down(game, added_merge=0)>,\n",
       " 2: <function src.common.movements.Movements.left(game, added_merge=0)>,\n",
       " 3: <function src.common.movements.Movements.right(game, added_merge=0)>}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  0.,  4.,  0.],\n",
       "       [ 2., 16.,  2.,  0.],\n",
       "       [ 8.,  8.,  8.,  0.],\n",
       "       [ 8., 64., 16.,  4.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 2., 16.,  4.,  0.],\n",
       "        [ 4.,  8.,  2.,  0.],\n",
       "        [ 4., 32.,  8.,  2.],\n",
       "        [ 8., 32., 16.,  2.]]),\n",
       " array([[ 2.,  0.,  4.,  0.],\n",
       "        [ 2., 16.,  2.,  0.],\n",
       "        [ 8.,  8.,  8.,  0.],\n",
       "        [ 8., 64., 16.,  4.]]),\n",
       " True,\n",
       " -2.752072486556415,\n",
       " 0)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._calc_penalty_max_position()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log2(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  0.,  0.,  4.],\n",
       "       [ 2.,  4.,  2.,  4.],\n",
       "       [ 0.,  8., 32.,  8.],\n",
       "       [ 4.,  8.,  2., 16.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = np.array([[ 2.,  0.,  0.,  4.],\n",
    "        [ 2.,  4.,  2.,  4.],\n",
    "        [ 0.,  8., 32.,  8.],\n",
    "        [ 4.,  8.,  2., 16.]])\n",
    "m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.,  4.,  2.,  8.],\n",
       "       [ 4., 16., 32.,  8.],\n",
       "       [ 0.,  0.,  2., 16.],\n",
       "       [ 0.,  0.,  2.,  0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2 = np.array([[ 4.,  4.,  2.,  8.],\n",
    "        [ 4., 16., 32.,  8.],\n",
    "        [ 0.,  0.,  2., 16.],\n",
    "        [ 0.,  0.,  2.,  0.]])\n",
    "m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = np.where(m2 == m2.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0.5, 0.5, 0. ],\n",
       "       [0.5, 1. , 1. , 0.5],\n",
       "       [0.5, 1. , 1. , 0.5],\n",
       "       [0. , 0.5, 0.5, 0. ]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(m2) * mask_position_val[pos[0], pos[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0625,  0.125 ,  0.0625,  0.125 ],\n",
       "       [ 0.0625,  0.375 ,  0.9375,  0.125 ],\n",
       "       [ 0.    , -0.25  , -0.9375,  0.25  ],\n",
       "       [-0.125 , -0.25  ,  0.    , -0.5   ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(m2 - m1) / np.max(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penalty_cell_move = c.GRID_LEN**2 - np.sum(m1 == m2)\n",
    "penalty_cell_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.807354922057604"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collapsed_cells = np.log2(28) if 28>0 else 0\n",
    "collapsed_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 4.,  4.,  2.,  8.],\n",
       "        [ 4., 16., 32.,  8.],\n",
       "        [ 0.,  0.,  2., 16.],\n",
       "        [ 0.,  0.,  0.,  0.]]),\n",
       " True,\n",
       " 3,\n",
       " 28.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.actions[0](m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14 - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
